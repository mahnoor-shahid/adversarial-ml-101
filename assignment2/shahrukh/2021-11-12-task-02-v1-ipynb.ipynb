{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ML in Cybersecurity: Task II\n\n## Team\n  * **Team name**:  *R2D2C3P0BB8*\n  * **Members**:  <br/> **Navdeeppal Singh (s8nlsing@stud.uni-saarland.de)** <br/> **Shahrukh Khan (shkh00001@stud.uni-saarland.de)** <br/> **Mahnoor Shahid (mash00001@stud.uni-saarland.de)**\n\n\n## Logistics\n  * **Due date**: 25th Nov. 2021, 23:59:59 (email the completed notebook including outputs to mlcysec_ws2022_staff@lists.cispa.saarland)\n  * Email the completed notebook to mlcysec_ws2022_staff@lists.cispa.saarland \n  * Complete this in the previously established **teams of 3**\n  * Feel free to use the course forum to discuss.\n  \n  \n## About this Project\nIn this project, we dive into the vulnerabilities of machine learning models and the difficulties of defending against them. To this end, we ask you to implement an evasion attack (craft adversarial examples) yourselves, and defend your own model.   \n\n\n## A Note on Grading\nThe total number of points in this project is 100. We further provide the number of points achievable with each excercise. You should take particular care to document and visualize your results.\n\nWhenever possible, please use tools like tables or figures to compare the different findings\n\n\n \n## Filling-in the Notebook\nYou'll be submitting this very notebook that is filled-in with (all) your code and analysis. Make sure you submit one that has been previously executed in-order. (So that results/graphs are already visible upon opening it). \n\nThe notebook you submit **should compile** (or should be self-contained and sufficiently commented). Check tutorial 1 on how to set up the Python3 environment.\n\nIt is extremely important that you **do not** re-order the existing sections. Apart from that, the code blocks that you need to fill-in are given by:\n```\n#\n#\n# ------- Your Code -------\n#\n#\n```\nFeel free to break this into multiple-cells. It's even better if you interleave explanations and code-blocks so that the entire notebook forms a readable \"story\".\n\n\n## Code of Honor\nWe encourage discussing ideas and concepts with other students to help you learn and better understand the course content. However, the work you submit and present **must be original** and demonstrate your effort in solving the presented problems. **We will not tolerate** blatantly using existing solutions (such as from the internet), improper collaboration (e.g., sharing code or experimental data between groups) and plagiarism. If the honor code is not met, no points will be awarded.\n\n \n  ---","metadata":{"id":"4NzyqehHjUlG"}},{"cell_type":"code","source":"import time \n \nimport numpy as np \nimport matplotlib.pyplot as plt \n\nimport json \nimport time \nimport pickle \nimport sys \nimport csv \nimport os \nimport os.path as osp \nimport shutil \nfrom collections import namedtuple\nimport pandas as pd\n\nfrom IPython.display import display, HTML\n \n%matplotlib inline \nplt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots \nplt.rcParams['image.interpolation'] = 'nearest' \nplt.rcParams['image.cmap'] = 'gray' \n \n# for auto-reloading external modules \n# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython \n%load_ext autoreload\n%autoreload 2","metadata":{"id":"3ewNwfFvbFaR","execution":{"iopub.status.busy":"2021-11-16T13:10:38.945523Z","iopub.execute_input":"2021-11-16T13:10:38.945955Z","iopub.status.idle":"2021-11-16T13:10:39.015552Z","shell.execute_reply.started":"2021-11-16T13:10:38.945796Z","shell.execute_reply":"2021-11-16T13:10:39.014590Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Some suggestions of our libraries that might be helpful for this project\nfrom collections import Counter          # an even easier way to count\nfrom multiprocessing import Pool         # for multiprocessing\nfrom tqdm import tqdm                    # fancy progress bars\n\n# Load other libraries here.\n# Keep it minimal! We should be easily able to reproduce your code.\n# We only support sklearn and pytorch.\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nimport torch.utils.data as data\nfrom sklearn.metrics import plot_confusion_matrix, accuracy_score, classification_report\n\n# We preload pytorch as an example\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset, random_split, SubsetRandomSampler\n# Please set random seed to have reproduceable results, e.g. torch.manual_seed(123)\nrandom_seed = 42\ntorch.manual_seed(random_seed)\nnp.random.seed(random_seed)","metadata":{"id":"640GrzbOevr0","execution":{"iopub.status.busy":"2021-11-16T13:10:39.686087Z","iopub.execute_input":"2021-11-16T13:10:39.686988Z","iopub.status.idle":"2021-11-16T13:10:42.515603Z","shell.execute_reply.started":"2021-11-16T13:10:39.686950Z","shell.execute_reply":"2021-11-16T13:10:42.514444Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"compute_mode = 'gpu'\n\nif compute_mode == 'cpu':\n    device = torch.device('cpu')\nelif compute_mode == 'gpu':\n    # If you are using pytorch on the GPU cluster, you have to manually specify which GPU device to use\n    # It is extremely important that you *do not* spawn multi-GPU jobs.\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'    # Set device ID here\n    device = torch.device('cuda')\nelse:\n    raise ValueError('Unrecognized compute mode')","metadata":{"id":"GJZPEAWYMhYB","execution":{"iopub.status.busy":"2021-11-16T13:10:42.517604Z","iopub.execute_input":"2021-11-16T13:10:42.517937Z","iopub.status.idle":"2021-11-16T13:10:42.586332Z","shell.execute_reply.started":"2021-11-16T13:10:42.517893Z","shell.execute_reply":"2021-11-16T13:10:42.584915Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"#### Helpers\n\nIn case you choose to have some methods you plan to reuse during the notebook, define them here. This will avoid clutter and keep rest of the notebook succinct.","metadata":{"id":"nxi-lLD0mKHD"}},{"cell_type":"code","source":"# data loading helper\ndef _get_data(DATA_PATH, TRAIN_BATCH_SIZE, TEST_BATCH_SIZE):\n    try:\n        \"\"\"\n        This method is created to split the MNIST data into training, validation and testing set accordingly \n        and load it into dataloaders. Also, to specify any transformations required to perform on the data. \n        As well as this method is being called multiple times in hyper parameter tuning where different batch \n        sizes are being tested\n        ...\n\n        Parameters\n        ----------\n        DATA_PATH : str\n            specifies the path directory where dataset will be downloaded\n        TRAIN_BATCH_SIZE : int\n            specifies the batch size in the training loader\n        TEST_BATCH_SIZE : int\n            specifies the batch size in the training loader\n            \n        Returns\n        -------\n        train_loader, validation_loader, test_loader with the specified batch sizes \n            \n        \"\"\"\n        tranformations = transforms.Compose([transforms.ToTensor()])\n        \n        mnist_training_dataset = datasets.MNIST(root=DATA_PATH+'train', train=True, download=True, transform=tranformations)\n        mnist_testing_dataset = datasets.MNIST(root=DATA_PATH+'test', train=False, download=True, transform=tranformations)\n        \n        training_dataset, validation_dataset = random_split(mnist_training_dataset, [int(0.8*len(mnist_training_dataset)), int(0.2*len(mnist_training_dataset))])\n        \n        train_loader = DataLoader(training_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n        validation_loader = DataLoader(training_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=False)\n        test_loader = DataLoader(mnist_testing_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)\n        \n        return train_loader, validation_loader, test_loader\n    \n    except Exception as e:\n        print('Unable to get data due to ', e)\n\n## data sample helper\ndef _get_test_data_sample(DATA_PATH, TEST_BATCH_SIZE=1, SAMPLE_SIZE=1000):\n    try:\n        \"\"\"\n        This method creates a subset from testset for creating adversarial examples\n        ...\n\n        Parameters\n        ----------\n        DATA_PATH : str\n            specifies the path directory where dataset will be downloaded\n        TRAIN_BATCH_SIZE : int\n            specifies the batch size in the test subset loader\n        SAMPLE_SIZE : int\n            specifies the sample size of the subset\n            \n        Returns\n        -------\n        test_subset_loader\n            \n        \"\"\"\n        tranformations = transforms.Compose([transforms.ToTensor()])\n        \n        mnist_testing_dataset = datasets.MNIST(root=DATA_PATH+'test', \n                                              train=False, download=True, \n                                              transform=tranformations)\n        dataset_size = len(mnist_testing_dataset)\n        dataset_indices = list(range(dataset_size))\n        np.random.shuffle(dataset_indices)\n        test_subset_idx  = dataset_indices[:SAMPLE_SIZE]\n        test_subset_sampler = SubsetRandomSampler(test_subset_idx)\n        test_subset_loader = DataLoader(mnist_testing_dataset, \n                                        batch_size=TEST_BATCH_SIZE, \n                                        shuffle=False, sampler=test_subset_sampler)\n        \n        return test_subset_loader\n    \n    except Exception as e:\n        print('Unable to get data due to ', e)\n\n","metadata":{"id":"VBbigqdEmKd8","execution":{"iopub.status.busy":"2021-11-16T13:10:43.035500Z","iopub.execute_input":"2021-11-16T13:10:43.035806Z","iopub.status.idle":"2021-11-16T13:10:43.092211Z","shell.execute_reply.started":"2021-11-16T13:10:43.035773Z","shell.execute_reply":"2021-11-16T13:10:43.090924Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working\n!mkdir ../data","metadata":{"execution":{"iopub.status.busy":"2021-11-16T13:10:43.554672Z","iopub.execute_input":"2021-11-16T13:10:43.555336Z","iopub.status.idle":"2021-11-16T13:10:44.438231Z","shell.execute_reply.started":"2021-11-16T13:10:43.555301Z","shell.execute_reply":"2021-11-16T13:10:44.436787Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!wget https://transfer.sh/6sYjXn/Accuracy_99.8875_batchsize_64_lr_0.001.ckpt","metadata":{"execution":{"iopub.status.busy":"2021-11-16T13:10:44.956991Z","iopub.execute_input":"2021-11-16T13:10:44.957314Z","iopub.status.idle":"2021-11-16T13:10:48.156996Z","shell.execute_reply.started":"2021-11-16T13:10:44.957279Z","shell.execute_reply":"2021-11-16T13:10:48.155882Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# 1. Attacking an ML-model (30 points) \n\nIn this section, we implement an attack ourselves. First, however, you need a model you can attack. Feel free to choose the DNN/ConvNN from task 1.\n\n","metadata":{"id":"n1pcmKkyjT7y"}},{"cell_type":"markdown","source":"## 1.1: Setting up the model and data (4 Points)\n\nLoad the MNIST data, as done in task 1. \n\nRe-use the model from task 1 here and train it until it achieves reasonable accuracy (>92%).\n\nIf you have the saved checkpoint from task 1, you can load it directly. But please compute here the test accuracy using this checkpoint.  \n\n**Hint:** In order to save computation time for the rest of exercise, you might consider having a relatively small model here.\n\n**Hint**: You might want to save the trained model to save time later.","metadata":{"id":"QaJv_d_Dp7OM"}},{"cell_type":"code","source":"## 1. Loading data\n\nDATA_PATH = '../data/'\nMODEL_PATH = './Accuracy_99.8875_batchsize_64_lr_0.001.ckpt'\nTRAIN_BATCH_SIZE, TEST_BATCH_SIZE = 64, 64\ntrain_loader, validation_loader, test_loader = _get_data(DATA_PATH, TRAIN_BATCH_SIZE, TEST_BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T13:41:51.616727Z","iopub.execute_input":"2021-11-16T13:41:51.617264Z","iopub.status.idle":"2021-11-16T13:41:51.756251Z","shell.execute_reply.started":"2021-11-16T13:41:51.617215Z","shell.execute_reply":"2021-11-16T13:41:51.752940Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"## 2. Defining model\n\nclass CNN_Network(nn.Module):\n    def __init__(self, model_params):\n        \"\"\"\n        This class is created to specify the Convolutional Neural Network on which MNIST dataset is trained on, \n        validated and later tested. \n        It consist of one input layer, one output layer can consist of multiple hidden layers all of which is \n        specified by the user as provided through model_paramaters\n        Size of the kernel, stride and padding can also be adjusted by the user as provided through model_paramaters\n        ...\n\n        Parameters\n        ----------\n        model_params : dictionary\n            provides the model with the required input size, hidden layers and output size\n            \n            model_params = {\n            'INPUT_SIZE' : int,\n            'HIDDEN_LAYERS' : list(int),\n            'OUTPUT_SIZE' : int,\n            'KERNEL' : int,\n            'STRIDE' : int,\n            'PADDING' : int\n        }\n        \"\"\"\n        try:\n            super(CNN_Network, self).__init__()\n            \n            layers = []\n            \n            for input_channel, out_channel in zip([model_params['INPUT_SIZE']] + model_params['HIDDEN_LAYERS'][:-1], \n                                                     model_params['HIDDEN_LAYERS'][:len(model_params['HIDDEN_LAYERS'])]):\n                layers.append(nn.Conv2d(input_channel, out_channel, model_params['KERNEL'], model_params['STRIDE'], model_params['PADDING'], bias=True))\n                layers.append(nn.MaxPool2d(2, 2))\n                layers.append(nn.ReLU())\n            layers.append(nn.Flatten(1))      \n            layers.append(nn.Linear(model_params['HIDDEN_LAYERS'][-1], model_params['OUTPUT_SIZE'], bias=True))\n\n            self.layers = nn.Sequential(*layers)\n        \n        except Exception as e:\n            print('initializing failed due to ', e)\n    \n    def forward(self, x):\n        try:\n            return self.layers(x)\n        \n        except Exception as e:\n            print('forward pass failed due to ', e)\n      ","metadata":{"execution":{"iopub.status.busy":"2021-11-16T13:10:53.728384Z","iopub.execute_input":"2021-11-16T13:10:53.734189Z","iopub.status.idle":"2021-11-16T13:10:53.822222Z","shell.execute_reply.started":"2021-11-16T13:10:53.734136Z","shell.execute_reply":"2021-11-16T13:10:53.820627Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"## 3. initializing the pre-trained model from assignment 1\nmodel_params = {\n        'INPUT_SIZE' : 1,\n        'HIDDEN_LAYERS' : [160, 100, 64, 10],\n        'OUTPUT_SIZE' : 10,\n        'KERNEL' : 3,\n        'STRIDE' : 1,\n        'PADDING' : 1\n}\ncriterion = nn.CrossEntropyLoss()\nundefended_model = CNN_Network(model_params).to(device)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T13:11:48.579050Z","iopub.execute_input":"2021-11-16T13:11:48.579389Z","iopub.status.idle":"2021-11-16T13:11:48.692323Z","shell.execute_reply.started":"2021-11-16T13:11:48.579339Z","shell.execute_reply":"2021-11-16T13:11:48.687219Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# loading checkpoint and evaluating on test set\ndef _test_model(model, test_loader, BEST_MODEL):\n    try:\n        model.load_state_dict(torch.load(BEST_MODEL, map_location=device))\n        model.eval()\n        with torch.no_grad():\n            correct_predictions = []\n            testing_acc_scores = []\n            wrong_predictions = []\n            all_targets = []\n            all_preds = []\n\n\n            for images, targets in iter(test_loader):\n                images = images.to(device)\n                targets = targets.to(device)\n                outputs = model(images)\n                \n                _, preds = torch.max(outputs, 1)\n                correct_indicies = (preds == targets).nonzero(as_tuple=True)[0]\n                c_images = images[correct_indicies]\n                c_targets = targets[correct_indicies]\n                c_correct_preds = preds[correct_indicies]\n                testing_acc_scores.append(len(correct_indicies)/targets.shape[0])\n\n                wrong_indicies = (preds != targets).nonzero(as_tuple=True)[0]\n                w_images = images[wrong_indicies]\n                w_targets = targets[wrong_indicies]\n                w_wrong_preds = preds[wrong_indicies]\n            \n                correct_predictions += zip(c_images, c_targets, c_correct_preds)\n                wrong_predictions += zip(w_images, w_targets, w_wrong_preds)\n                all_targets+= zip(targets.cpu().numpy())\n                all_preds+= zip(preds.cpu().numpy())\n\n            return (sum(testing_acc_scores)/len(testing_acc_scores))*100, correct_predictions, wrong_predictions, all_targets, all_preds\n        \n    except Exception as e:\n            print('Error occured in testing the model = ', e)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T13:11:48.827041Z","iopub.execute_input":"2021-11-16T13:11:48.827825Z","iopub.status.idle":"2021-11-16T13:11:48.917316Z","shell.execute_reply.started":"2021-11-16T13:11:48.827777Z","shell.execute_reply":"2021-11-16T13:11:48.915826Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"(test_accuracy, \n correct_predictions, \n wrong_predictions, \n all_targets, all_preds) = _test_model(undefended_model, test_loader, \n                                       BEST_MODEL=MODEL_PATH )","metadata":{"execution":{"iopub.status.busy":"2021-11-16T13:11:49.036146Z","iopub.execute_input":"2021-11-16T13:11:49.037162Z","iopub.status.idle":"2021-11-16T13:11:50.830777Z","shell.execute_reply.started":"2021-11-16T13:11:49.037073Z","shell.execute_reply":"2021-11-16T13:11:50.829738Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"print(classification_report(all_targets, all_preds))","metadata":{"execution":{"iopub.status.busy":"2021-11-16T13:11:50.833164Z","iopub.execute_input":"2021-11-16T13:11:50.833467Z","iopub.status.idle":"2021-11-16T13:11:50.987774Z","shell.execute_reply.started":"2021-11-16T13:11:50.833422Z","shell.execute_reply":"2021-11-16T13:11:50.986744Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## 1.2: Implementing the FGSM attack (7 Points)\n\nWe now want to attack the model trained in the previous step. We will start with the FGSM attack as a simple example. \n\nPlease implement the FGSM attack mentioned in the lecture. \n\nMore details: https://arxiv.org/pdf/1412.6572.pdf\n","metadata":{"id":"DEQrdyLHsUIu"}},{"cell_type":"code","source":"#implementation of FGSM Attack\ndef fgsm_attack(example, epsilon, x_grad):\n    \"\"\"\n    Implementation of FGSM attack per bacth\n    example: Input image batch\n    epsilon: Pertubation budget \n    x_grad: Gradient of loss function wrt input image\n    \"\"\"\n    # computing the sign (elementwise) of input (x) gradient\n    x_grad_sign = x_grad.sign()\n    # generating adversarial example using FGSM formula of x + delta, where delta = epsilon * input_gradient_sign\n    perturbed_example = example + epsilon*x_grad_sign\n    # perform house keeping to make sure image pixels don't go out of limits of [0,1] range\n    clipped_perturbed_example = torch.clamp(perturbed_example, 0, 1)\n    # return the adversarial example \n    return clipped_perturbed_example","metadata":{"id":"gcVZnUNbRKOz","execution":{"iopub.status.busy":"2021-11-16T13:11:50.989694Z","iopub.execute_input":"2021-11-16T13:11:50.990288Z","iopub.status.idle":"2021-11-16T13:11:51.036940Z","shell.execute_reply.started":"2021-11-16T13:11:50.990244Z","shell.execute_reply":"2021-11-16T13:11:51.035725Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## 1.3: Adversarial sample set (7 Points)\n\n* Please generate a dataset containing at least 1,000 adversarial examples using FGSM.\n\n* Please vary the perturbation budget (3 variants) and generate 1,000 adversarial examples for each. \n    * **Hint**: you can choose epsilons within, e.g., = [.05, .1, .15, .2, .25, .3],  using MNIST pixel values in the interval       [0, 1]\n\n* Compute the accuracy of each attack set. ","metadata":{"id":"RNpI3oUoO1wE"}},{"cell_type":"code","source":"## dictionary for storing 3 sets of adversarial examples\nfgsm_sets = {}\n## epsilon values\nepsilons = [0.1, 0.2, 0.3]\nepsilon_param = namedtuple('epsilon_param', ['epsilon'])\n\n# creating the 1000 example dataset first\ntest_subset_dataloader = _get_test_data_sample(DATA_PATH)","metadata":{"id":"EvYpo9p2O1wF","execution":{"iopub.status.busy":"2021-11-16T13:11:52.522101Z","iopub.execute_input":"2021-11-16T13:11:52.522460Z","iopub.status.idle":"2021-11-16T13:11:52.588167Z","shell.execute_reply.started":"2021-11-16T13:11:52.522412Z","shell.execute_reply":"2021-11-16T13:11:52.587103Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"## creating adversarial datasets using FGSM pertubations for epsilons = [0.1, 0.2, 0.3]\nfor epsilon in tqdm(epsilons):\n    epsilon_val = epsilon_param(epsilon=epsilon)\n    fgsm_sets[epsilon_val] = {\n         \"fgsm_images\": [],\n         \"targets\": [],\n        \"undefended_model_predictions\": []\n     }\n    for x_data, target in test_subset_dataloader:\n        # move inputs and labels to the compute device\n        x_data= x_data.to(device) \n        target = target.to(device)\n        # switch on the gradient computations wrt to input, as need those computations for FGSM formula\n        x_data.requires_grad = True\n        # peform the forward pass of the model\n        output = undefended_model(x_data)\n        #print(output)\n        # compute the loss\n        loss = criterion(output, target)\n        # re-initialized all the gradient variables\n        undefended_model.zero_grad()\n        # perform the backward pass on loss function wrt to inputs and weights\n        loss.backward()\n        # retrieve gradients wrt to input\n        x_grad = x_data.grad.data\n\n        # perform the FGSM computation for generating adversarial example for a given epsilon\n        fgsm_output = fgsm_attack(x_data, epsilon, x_grad)\n        fgsm_sets[epsilon_val][\"fgsm_images\"].append(fgsm_output)\n        fgsm_sets[epsilon_val][\"targets\"].append(target.cpu().numpy())\n        fgsm_sets[epsilon_val][\"undefended_model_predictions\"].append(output.max(1, keepdim=True)[1][0].cpu().numpy())","metadata":{"execution":{"iopub.status.busy":"2021-11-16T13:11:54.214410Z","iopub.execute_input":"2021-11-16T13:11:54.214703Z","iopub.status.idle":"2021-11-16T13:12:06.517138Z","shell.execute_reply.started":"2021-11-16T13:11:54.214669Z","shell.execute_reply":"2021-11-16T13:12:06.515914Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"for epsilon in tqdm(epsilons):\n    epsilon_val = epsilon_param(epsilon=epsilon)\n    fgsm_sets[epsilon_val][\"undefended_model_fgsm_predictions\"] = []\n    for index, perturbed_image in enumerate(fgsm_sets[epsilon_val][\"fgsm_images\"]):\n        perturbed_image.to(device)\n        output = undefended_model(fgsm_sets[epsilon_val][\"fgsm_images\"][index])\n        preds = torch.max(output, 1)[1]\n        \n        fgsm_sets[epsilon_val][\"undefended_model_fgsm_predictions\"].append(preds.cpu().numpy())\n    print('\\n')\n    print(f\"Accuracy report for perturbation budget: {epsilon}\")\n    print(classification_report(fgsm_sets[epsilon_val][\"targets\"], fgsm_sets[epsilon_val][\"undefended_model_fgsm_predictions\"]))\n    print('\\n')","metadata":{"execution":{"iopub.status.busy":"2021-11-16T13:12:10.644095Z","iopub.execute_input":"2021-11-16T13:12:10.644908Z","iopub.status.idle":"2021-11-16T13:12:14.578131Z","shell.execute_reply.started":"2021-11-16T13:12:10.644828Z","shell.execute_reply":"2021-11-16T13:12:14.577070Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## 1.4: Visualizing the results (7 Points)\n\n* Please chose one sample for each class (for example the first when iterating the test data) and plot the (ten) adversarial examples as well as the predicted label (before and after the attack)\n\n* Please repeat the visualization for the three sets you have created ","metadata":{"id":"Ex3qQp3JolD1"}},{"cell_type":"code","source":"def visualize_perturbed_samples(epsilon):\n    nsamples = 10\n    nrows = nsamples\n    ncols = 10\n\n    fig, axes = plt.subplots(1,ncols,figsize=(1.5*nrows,2.5*ncols))  # create the figure with subplots\n    [ax.set_axis_off() for ax in axes.ravel()]  # remove the axis\n\n    epsilon_val = epsilon_param(epsilon=epsilon)\n    labels = np.array([target[0] for target in fgsm_sets[epsilon_val][\"targets\"]])\n    for i in range(nsamples):\n        index = result = np.where(labels == i)[0][0]\n        axes[i].imshow(fgsm_sets[epsilon_val][\"fgsm_images\"][index].detach().cpu().numpy()[0][0])\n        axes[i].set_title('before \\n attack: {} \\n after \\n attack: {}'.format(fgsm_sets[epsilon_val][\"undefended_model_predictions\"][index][0],\n                                                                                fgsm_sets[epsilon_val][\"undefended_model_fgsm_predictions\"][index][0]))\n","metadata":{"id":"eGkp0B0PO1wJ","execution":{"iopub.status.busy":"2021-11-16T13:12:25.769060Z","iopub.execute_input":"2021-11-16T13:12:25.769389Z","iopub.status.idle":"2021-11-16T13:12:25.821456Z","shell.execute_reply.started":"2021-11-16T13:12:25.769355Z","shell.execute_reply":"2021-11-16T13:12:25.819957Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing examples for epsilon=0.1","metadata":{}},{"cell_type":"code","source":"visualize_perturbed_samples(epsilon=0.1)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T13:12:27.508506Z","iopub.execute_input":"2021-11-16T13:12:27.508801Z","iopub.status.idle":"2021-11-16T13:12:28.321111Z","shell.execute_reply.started":"2021-11-16T13:12:27.508770Z","shell.execute_reply":"2021-11-16T13:12:28.319951Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing examples for epsilon=0.2","metadata":{}},{"cell_type":"code","source":"visualize_perturbed_samples(epsilon=0.2)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T13:12:28.744405Z","iopub.execute_input":"2021-11-16T13:12:28.744690Z","iopub.status.idle":"2021-11-16T13:12:29.402787Z","shell.execute_reply.started":"2021-11-16T13:12:28.744658Z","shell.execute_reply":"2021-11-16T13:12:29.401837Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing examples for epsilon=0.3","metadata":{}},{"cell_type":"code","source":"visualize_perturbed_samples(epsilon=0.3)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T13:12:30.206059Z","iopub.execute_input":"2021-11-16T13:12:30.209111Z","iopub.status.idle":"2021-11-16T13:12:30.907399Z","shell.execute_reply.started":"2021-11-16T13:12:30.209062Z","shell.execute_reply":"2021-11-16T13:12:30.906401Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## 1.5: Analyzing the results (5 Points)\n\nPlease write a brief summary of your findings.  \n\n* Does the attack always succeed (the model makes wrong prediction on the adversarial sample)? What is the relationship between the attack success rate and the perturbation budget?\n* How about the computation cost of the attack? (you can report the time in second) \n* Does the attack require white-box access to the model?\n* Feel free to report your results via tables or figures, and mention any other interesting observations \n\n","metadata":{}},{"cell_type":"markdown","source":"**Your answers go here**","metadata":{}},{"cell_type":"markdown","source":"# 2. Defending an ML model (35 points) \n\nSo far, we have focused on attacking an ML model. In this section, we want you to defend your model. \n","metadata":{"id":"KJUmrv5Bymij"}},{"cell_type":"markdown","source":"## 2.1: Implementing the adversarial training defense (20 Points)\n\n* We would like to ask you to implement the adversarial training defense (https://arxiv.org/pdf/1412.6572.pdf) mentioned in the lecture. \n\n* You can use the **FGSM adversarial training** method (i.e., train on FGSM examples). \n\n* You can also check the adversarial training implementation in other papers, e.g., http://proceedings.mlr.press/v97/pang19a/pang19a.pdf \n\n* Choose a certain **maximum perturbation budget** during training that is in the middle of the range you have experimented with before. \n\n* We do not require the defense to work perfectly - but what we want you to understand is why it works or why it does not work.\n\n**Hint:** You can save the checkpoint of the defended model as we would need it to for the third part of this exercise.\n","metadata":{"id":"0gHUFK6Mymik"}},{"cell_type":"code","source":"## We use the same hyperparameter selection that we selected from task 1 and then start adversarial training\ndefended_model = CNN_Network(model_params).to(device)","metadata":{"id":"DD0UalSeymim","execution":{"iopub.status.busy":"2021-11-16T13:33:55.544049Z","iopub.execute_input":"2021-11-16T13:33:55.544354Z","iopub.status.idle":"2021-11-16T13:33:55.600083Z","shell.execute_reply.started":"2021-11-16T13:33:55.544324Z","shell.execute_reply":"2021-11-16T13:33:55.598660Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"## defining loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.RMSprop(defended_model.parameters(), lr=0.001)\nepochs = 10\nmax_perturbation_budget = 0.2 ## pertubation budget\nalpha = 0.5 ## weight term for commulative loss for adversarial training","metadata":{"execution":{"iopub.status.busy":"2021-11-16T13:33:55.910051Z","iopub.execute_input":"2021-11-16T13:33:55.910387Z","iopub.status.idle":"2021-11-16T13:33:55.956370Z","shell.execute_reply.started":"2021-11-16T13:33:55.910354Z","shell.execute_reply":"2021-11-16T13:33:55.955224Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"best_accuracy = 0\ntraining_loss_history = []\nvalidation_loss_history = []\ntraining_acc_history = []\nvalidation_acc_history = []\nfor epoch in range(0, epochs):\n    defended_model.train()\n    train_loss_scores = []\n    training_acc_scores = []\n    correct_predictions = 0\n\n    for batch_index, (images, targets) in enumerate(train_loader):\n        images = images.to(device)\n        targets = targets.to(device)\n        # switch on the gradient computations wrt to input, as need those computations for FGSM formula\n        images.requires_grad = True\n        ## perform the forward pass\n        initial_outputs = defended_model(images)\n        # compute the loss\n        initial_loss = criterion(initial_outputs, targets)\n        # re-initialized all the gradient variables\n        defended_model.zero_grad()\n        ## perform the backward pass\n        initial_loss.backward()\n        # retrieve gradients wrt to input\n        x_grad = images.grad.data\n        # perform the FGSM computation for generating adversarial example for a given epsilon\n        ## drawing value of epsilon from uniform distribution by setting upper bound as max_perturbation_budget=0.2\n        epsilon = np.random.uniform(low=0.01, high=max_perturbation_budget)\n        fgsm_output = fgsm_attack(images, epsilon, x_grad)\n        \n        ## perform the forward pass on adversarial and original mini batch\n        fgsm_images_outputs = defended_model(fgsm_output)\n        images_outputs = defended_model(images)\n        \n        # compute the losses on both batches\n        adversarial_loss = criterion(fgsm_images_outputs, targets)\n        original_loss = criterion(images_outputs, targets)\n        ## compute commulative loss\n        total_loss = (alpha * original_loss) + ((1-alpha)* adversarial_loss)\n        train_loss_scores.append(total_loss.item())\n\n        _, adv_preds = torch.max(fgsm_images_outputs, 1)\n        _, original_preds = torch.max(images_outputs, 1)\n        correct_predictions = (adv_preds==targets).sum().item() + (original_preds==targets).sum().item()\n        training_acc_scores.append(correct_predictions/(targets.shape[0]*2))\n\n        optimizer.zero_grad()\n        total_loss.backward()\n        optimizer.step()\n\n        if (batch_index+1) % 100 == 0:\n            print(f\"Epoch : [{epoch+1}/{epochs}] | Step : [{batch_index+1}/{len(train_loader)}] | Loss : {total_loss.item()} \")\n\n    training_loss_history.append((sum(train_loss_scores)/len(train_loss_scores)))\n    training_acc_history.append((sum(training_acc_scores)/len(training_acc_scores))*100)      \n    print(f'Epoch : {epoch+1} | Loss : {training_loss_history[-1]} | Training Accuracy : {training_acc_history[-1]}%')\n    \n    \n    correct_predictions = 0\n    validation_acc_scores = []\n    validation_loss_scores = []\n\n    for images, targets in iter(validation_loader):\n        images = images.to(device)\n        targets = targets.to(device)\n        # switch on the gradient computations wrt to input, as need those computations for FGSM formula\n        images.requires_grad = True\n        ## perform the forward pass\n        initial_outputs = defended_model(images)\n        # compute the loss\n        initial_loss = criterion(initial_outputs, targets)\n        # re-initialized all the gradient variables\n        defended_model.zero_grad()\n        ## perform the backward pass\n        initial_loss.backward()\n        # retrieve gradients wrt to input\n        x_grad = images.grad.data\n        # perform the FGSM computation for generating adversarial example for a given epsilon\n        ## drawing value of epsilon from uniform distribution by setting upper bound as max_perturbation_budget=0.2\n        epsilon = np.random.uniform(low=0.01, high=max_perturbation_budget)\n        fgsm_output = fgsm_attack(images, epsilon, x_grad)\n\n        ## perform the forward pass on adversarial and original mini batch\n        fgsm_images_outputs = defended_model(fgsm_output)\n        images_outputs = defended_model(images)\n\n\n        _, adv_preds = torch.max(fgsm_images_outputs, 1)\n        _, original_preds = torch.max(images_outputs, 1)\n        correct_predictions = (adv_preds==targets).sum().item() + (original_preds==targets).sum().item()\n        validation_acc_scores.append(correct_predictions/(targets.shape[0]*2))\n\n\n    validation_acc_history.append((sum(validation_acc_scores)/len(validation_acc_scores))*100)\n    print(f'Epoch {epoch+1} | Validation Accuracy {validation_acc_history[-1]}%')\n\n    if validation_acc_history[-1]>best_accuracy:\n        best_accuracy = validation_acc_history[-1]\n        print('Saving the model...')\n        torch.save(defended_model.state_dict(), f\"best_cnn_adversarial_training.ckpt\")\n","metadata":{"execution":{"iopub.status.busy":"2021-11-16T13:43:53.043420Z","iopub.execute_input":"2021-11-16T13:43:53.043731Z","iopub.status.idle":"2021-11-16T13:48:53.747273Z","shell.execute_reply.started":"2021-11-16T13:43:53.043700Z","shell.execute_reply":"2021-11-16T13:48:53.746017Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"## 2.2: Evaluation (10 Points)\n\n* Craft adversarial examples using the **defended** model. This entails at least 1,000 examples crafted via FGSM. \n    * Create one set using a budget that is **less than (within)** the one used in training.\n    * Create another set using a budget that is **higher than** the one used in training. \n    * You can use two values of epsilons from question 1.3 \n    \n* Evaluate the **defended** model on these two adversarial examples sets. \n","metadata":{}},{"cell_type":"code","source":"## dictionary for storing 2 sets of adversarial examples with 1000 examples each\nfgsm_defended_model_sets = {}\n## using epsilon values of 0.1 and 0.3 since the max_perturbation_budget was 0.2 so 0.1 is within the range and 0.3 is greater than that\nepsilons_defended = [0.1, 0.3]\nepsilon_param = namedtuple('epsilon_param', ['epsilon'])\n\n# creating the 1000 example dataset first\ntest_subset_dataloader = _get_test_data_sample(DATA_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T13:54:17.826879Z","iopub.execute_input":"2021-11-16T13:54:17.827445Z","iopub.status.idle":"2021-11-16T13:54:17.882903Z","shell.execute_reply.started":"2021-11-16T13:54:17.827396Z","shell.execute_reply":"2021-11-16T13:54:17.881756Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"## creating adversarial datasets using FGSM pertubations for epsilons = [0.1, 0.3] using the DEFENDED MODEL\nfor epsilon in tqdm(epsilons_defended):\n    epsilon_val = epsilon_param(epsilon=epsilon)\n    fgsm_defended_model_sets[epsilon_val] = {\n         \"fgsm_images\": [],\n         \"targets\": [],\n        \"defended_model_predictions\": []\n     }\n    for x_data, target in test_subset_dataloader:\n        # move inputs and labels to the compute device\n        x_data= x_data.to(device) \n        target = target.to(device)\n        # switch on the gradient computations wrt to input, as need those computations for FGSM formula\n        x_data.requires_grad = True\n        # peform the forward pass of the model\n        output = defended_model(x_data)\n        #print(output)\n        # compute the loss\n        loss = criterion(output, target)\n        # re-initialized all the gradient variables\n        defended_model.zero_grad()\n        # perform the backward pass on loss function wrt to inputs and weights\n        loss.backward()\n        # retrieve gradients wrt to input\n        x_grad = x_data.grad.data\n\n        # perform the FGSM computation for generating adversarial example for a given epsilon\n        fgsm_output = fgsm_attack(x_data, epsilon, x_grad)\n        fgsm_defended_model_sets[epsilon_val][\"fgsm_images\"].append(fgsm_output)\n        fgsm_defended_model_sets[epsilon_val][\"targets\"].append(target.cpu().numpy())\n        fgsm_defended_model_sets[epsilon_val][\"defended_model_predictions\"].append(output.max(1, keepdim=True)[1][0].cpu().numpy())","metadata":{"execution":{"iopub.status.busy":"2021-11-16T14:00:31.419443Z","iopub.execute_input":"2021-11-16T14:00:31.419714Z","iopub.status.idle":"2021-11-16T14:00:39.475636Z","shell.execute_reply.started":"2021-11-16T14:00:31.419685Z","shell.execute_reply":"2021-11-16T14:00:39.474519Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"for epsilon in tqdm(epsilons_defended):\n    epsilon_val = epsilon_param(epsilon=epsilon)\n    fgsm_defended_model_sets[epsilon_val][\"defended_model_fgsm_predictions\"] = []\n    for index, perturbed_image in enumerate(fgsm_defended_model_sets[epsilon_val][\"fgsm_images\"]):\n        perturbed_image.to(device)\n        output = defended_model(fgsm_defended_model_sets[epsilon_val][\"fgsm_images\"][index])\n        preds = torch.max(output, 1)[1]\n        \n        fgsm_defended_model_sets[epsilon_val][\"defended_model_fgsm_predictions\"].append(preds.cpu().numpy())\n    print('\\n')\n    if epsilon <= 0.2:\n        print(f\"Accuracy report for within perturbation budget epsilon: {epsilon}\")\n    else:\n        print(f\"Accuracy report for higher than perturbation budget epsilon: {epsilon}\")\n    print(classification_report(fgsm_defended_model_sets[epsilon_val][\"targets\"], fgsm_defended_model_sets[epsilon_val][\"defended_model_fgsm_predictions\"]))\n    print('\\n')","metadata":{"execution":{"iopub.status.busy":"2021-11-16T14:01:26.295700Z","iopub.execute_input":"2021-11-16T14:01:26.296269Z","iopub.status.idle":"2021-11-16T14:01:28.850483Z","shell.execute_reply.started":"2021-11-16T14:01:26.296234Z","shell.execute_reply":"2021-11-16T14:01:28.849308Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"## accuracy scores for epsilon=0.1\nacc_FGSM1 = accuracy_score(fgsm_sets[epsilon_param(epsilon=0.1)]['targets'],fgsm_sets[epsilon_param(epsilon=0.1)]['undefended_model_fgsm_predictions'])\nacc_FGSM_defend1 = accuracy_score(fgsm_defended_model_sets[epsilon_param(epsilon=0.1)]['targets'],fgsm_defended_model_sets[epsilon_param(epsilon=0.1)]['defended_model_fgsm_predictions'])\n\n## accuracy scores for epsilon=0.3\nacc_FGSM2 = accuracy_score(fgsm_sets[epsilon_param(epsilon=0.3)]['targets'],fgsm_sets[epsilon_param(epsilon=0.3)]['undefended_model_fgsm_predictions'])\nacc_FGSM_defend2 = accuracy_score(fgsm_defended_model_sets[epsilon_param(epsilon=0.3)]['targets'],fgsm_defended_model_sets[epsilon_param(epsilon=0.3)]['defended_model_fgsm_predictions'])","metadata":{"execution":{"iopub.status.busy":"2021-11-16T14:09:21.826781Z","iopub.execute_input":"2021-11-16T14:09:21.827103Z","iopub.status.idle":"2021-11-16T14:09:21.891451Z","shell.execute_reply.started":"2021-11-16T14:09:21.827070Z","shell.execute_reply":"2021-11-16T14:09:21.890443Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"#\n#\n# ------- Your Code -------\n#\n#\n\nprint('Accuracy on the lower-budget adversarial samples (FGSM) %.2f'%acc_FGSM1)\nprint('Accuracy on the lower-budget adversarial samples (FGSM) after defense %.2f'%acc_FGSM_defend1)\n\nprint('Accuracy on the higher-budget adversarial samples (FGSM) %.2f'%acc_FGSM2)\nprint('Accuracy on the higher-budget adversarial samples (FGSM) after defense %.2f'%acc_FGSM_defend2)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T14:09:22.344367Z","iopub.execute_input":"2021-11-16T14:09:22.344646Z","iopub.status.idle":"2021-11-16T14:09:22.392520Z","shell.execute_reply.started":"2021-11-16T14:09:22.344614Z","shell.execute_reply":"2021-11-16T14:09:22.391049Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":"## 2.3 Discussion (5 points)\n\n* How successful was the defense against the attack compared to the undefended model? How do you interpret the difference?\n* How did the two sets differ?","metadata":{}},{"cell_type":"markdown","source":"**Your answers go here**","metadata":{}},{"cell_type":"markdown","source":"# 3: I-FGSM attack (35 points) \n\n* FGSM is one of the simplest and earliest attacks. Since then, many more advanced attacks have been proposed. \n* One of them is the Iterative-FGSM (https://arxiv.org/pdf/1607.02533.pdf), where the attack is repeated multiple times.\n* In this part, we ask you to please implement the iterative FGSM attack. \n\n","metadata":{}},{"cell_type":"markdown","source":"## 3.1: Implementing the I-FGSM attack (10 Points)\n\n**Hints**: \n\n* Your code should have an attack loop. At each step, the FGSM attack that you have implemented before is computed using a small step.\n* After each step, you should perform a per-pixel clipping to make sure the image is in the allowed range, and that the perturbation is within budget.\n","metadata":{}},{"cell_type":"code","source":"#\n#\n# ------- Your Code -------\n#\n#\n","metadata":{"execution":{"iopub.status.busy":"2021-11-16T10:10:20.196669Z","iopub.status.idle":"2021-11-16T10:10:20.197581Z","shell.execute_reply.started":"2021-11-16T10:10:20.197319Z","shell.execute_reply":"2021-11-16T10:10:20.197344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2: Attack the undefended model (5 Points)\n\n* We will first attack the **undefended model** (i.e., without adversarial training).\n\n* Choose one perturbation budget from Question **1.3** for comparison. \n\n    * Hint: A simple way to choose the small step is to divide the total budget by the number of steps (e.g., 10).\n\n* Please generate 1000 adversarial examples using the **undefended** model and the **I-FGSM** you implemented. \n\n* Please compute the accuracy of the adversarial set on the **undefended** model. ","metadata":{}},{"cell_type":"code","source":"#\n#\n# ------- Your Code -------\n#\n#\n","metadata":{"execution":{"iopub.status.busy":"2021-11-16T10:10:20.198723Z","iopub.status.idle":"2021-11-16T10:10:20.199620Z","shell.execute_reply.started":"2021-11-16T10:10:20.199356Z","shell.execute_reply":"2021-11-16T10:10:20.199382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.1: Findings and comparison with FGSM (8 points)\n\n* Please report your findings. How successful was the attack? \n\n* What do you expect when increasing the number of steps? (you can experiment with different parameters of the attack and report your findings) \n\n* Compare with the basic FGSM. Using the same perturbation budget and using the same model, which attack is more successful? Why do you think this is the case? What about the computation time?\n\n* Feel free to report any interesting observations. ","metadata":{}},{"cell_type":"markdown","source":"**Your answers go here**","metadata":{}},{"cell_type":"markdown","source":"## 3.3: Attack the defended model (5 poinst) \n\n* In the previous question, we attacked the **undefended model**. \n\n* Now, we want to explore how successful the previous implemented defense (FGSM adversarial training) is againts this new attack. (we will not implement a new defense here, we will be reusing your previous checkpoint of the **defended model**)\n\n\n* Use the **defended model** to create one set of adversarial examples. Use a perturbation budget from Question **2.2** for comparison.  ","metadata":{}},{"cell_type":"code","source":"#\n#\n# ------- Your Code -------\n#\n#","metadata":{"execution":{"iopub.status.busy":"2021-11-16T10:10:20.200722Z","iopub.status.idle":"2021-11-16T10:10:20.201543Z","shell.execute_reply.started":"2021-11-16T10:10:20.201298Z","shell.execute_reply":"2021-11-16T10:10:20.201324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = torch.Tensor([1,2,3])\ntargets=torch.Tensor([3,2,1])\n(preds == targets).nonzero(as_tuple=True)[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-16T10:10:20.202650Z","iopub.status.idle":"2021-11-16T10:10:20.203435Z","shell.execute_reply.started":"2021-11-16T10:10:20.203191Z","shell.execute_reply":"2021-11-16T10:10:20.203216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3.1: Discussion (7 points) \n* Please report your results. How successful was the attack on the defended model? \n* Compare it with the success of the FGSM attack on the defended model. What do you observe? How do you interpret the difference? \n* How do you think you can improve the defense against I-FGSM attack?\n\n\n* Feel free to state any interesting findings you encountered during this project.","metadata":{}},{"cell_type":"markdown","source":"**Your answers go here**","metadata":{}}]}