{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4NzyqehHjUlG"
   },
   "source": [
    "# ML in Cybersecurity: Task II\n",
    "\n",
    "## Team\n",
    "  * **Team name**:  *R2D2C3P0BB8*\n",
    "  * **Members**:  <br/> **Navdeeppal Singh (s8nlsing@stud.uni-saarland.de)** <br/> **Shahrukh Khan (shkh00001@stud.uni-saarland.de)** <br/> **Mahnoor Shahid (mash00001@stud.uni-saarland.de)**\n",
    "\n",
    "\n",
    "## Logistics\n",
    "  * **Due date**: 25th Nov. 2021, 23:59:59 (email the completed notebook including outputs to mlcysec_ws2022_staff@lists.cispa.saarland)\n",
    "  * Email the completed notebook to mlcysec_ws2022_staff@lists.cispa.saarland \n",
    "  * Complete this in the previously established **teams of 3**\n",
    "  * Feel free to use the course forum to discuss.\n",
    "  \n",
    "  \n",
    "## About this Project\n",
    "In this project, we dive into the vulnerabilities of machine learning models and the difficulties of defending against them. To this end, we ask you to implement an evasion attack (craft adversarial examples) yourselves, and defend your own model.   \n",
    "\n",
    "\n",
    "## A Note on Grading\n",
    "The total number of points in this project is 100. We further provide the number of points achievable with each excercise. You should take particular care to document and visualize your results.\n",
    "\n",
    "Whenever possible, please use tools like tables or figures to compare the different findings\n",
    "\n",
    "\n",
    " \n",
    "## Filling-in the Notebook\n",
    "You'll be submitting this very notebook that is filled-in with (all) your code and analysis. Make sure you submit one that has been previously executed in-order. (So that results/graphs are already visible upon opening it). \n",
    "\n",
    "The notebook you submit **should compile** (or should be self-contained and sufficiently commented). Check tutorial 1 on how to set up the Python3 environment.\n",
    "\n",
    "It is extremely important that you **do not** re-order the existing sections. Apart from that, the code blocks that you need to fill-in are given by:\n",
    "```\n",
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "```\n",
    "Feel free to break this into multiple-cells. It's even better if you interleave explanations and code-blocks so that the entire notebook forms a readable \"story\".\n",
    "\n",
    "\n",
    "## Code of Honor\n",
    "We encourage discussing ideas and concepts with other students to help you learn and better understand the course content. However, the work you submit and present **must be original** and demonstrate your effort in solving the presented problems. **We will not tolerate** blatantly using existing solutions (such as from the internet), improper collaboration (e.g., sharing code or experimental data between groups) and plagiarism. If the honor code is not met, no points will be awarded.\n",
    "\n",
    " \n",
    "  ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ewNwfFvbFaR"
   },
   "outputs": [],
   "source": [
    "import time \n",
    " \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import json \n",
    "import time \n",
    "import pickle \n",
    "import sys \n",
    "import csv \n",
    "import os \n",
    "import os.path as osp \n",
    "import shutil \n",
    "import decimal\n",
    "import pandas as pd\n",
    "from sewar.full_ref import mse, rmse, psnr, uqi, ssim, ergas, scc, rase, sam, msssim, vifp, psnrb\n",
    "from IPython.display import display, HTML\n",
    " \n",
    "%matplotlib inline \n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots \n",
    "plt.rcParams['image.interpolation'] = 'nearest' \n",
    "plt.rcParams['image.cmap'] = 'gray' \n",
    " \n",
    "# for auto-reloading external modules \n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "640GrzbOevr0"
   },
   "outputs": [],
   "source": [
    "# Some suggestions of our libraries that might be helpful for this project\n",
    "from collections import Counter          # an even easier way to count\n",
    "from multiprocessing import Pool         # for multiprocessing\n",
    "from tqdm import tqdm                    # fancy progress bars\n",
    "import warnings\n",
    "import sklearn.metrics\n",
    "# Load other libraries here.\n",
    "# Keep it minimal! We should be easily able to reproduce your code.\n",
    "# We only support sklearn and pytorch.\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "\n",
    "# We preload pytorch as an example\n",
    "import torch\n",
    "\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split\n",
    "# Please set random seed to have reproduceable results, e.g. torch.manual_seed(123)\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GJZPEAWYMhYB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "compute_mode = 'gpu'\n",
    "global device\n",
    "\n",
    "if compute_mode == 'cpu':\n",
    "    device = torch.device('cpu')\n",
    "elif compute_mode == 'gpu':\n",
    "    # If you are using pytorch on the GPU cluster, you have to manually specify which GPU device to use\n",
    "    # It is extremely important that you *do not* spawn multi-GPU jobs.\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'    # Set device ID here\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    raise ValueError('Unrecognized compute mode')\n",
    "    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nxi-lLD0mKHD"
   },
   "source": [
    "#### Helpers\n",
    "\n",
    "In case you choose to have some methods you plan to reuse during the notebook, define them here. This will avoid clutter and keep rest of the notebook succinct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VBbigqdEmKd8"
   },
   "outputs": [],
   "source": [
    "# data loading helper\n",
    "def _get_data(DATA_PATH, TRAIN_BATCH_SIZE, TEST_BATCH_SIZE):\n",
    "    try:\n",
    "        \"\"\"\n",
    "        This method is created to split the MNIST data into training, validation and testing set accordingly \n",
    "        and load it into dataloaders. Also, to specify any transformations required to perform on the data. \n",
    "        As well as this method is being called multiple times in hyper parameter tuning where different batch \n",
    "        sizes are being tested\n",
    "        ...\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        DATA_PATH : str\n",
    "            specifies the path directory where dataset will be downloaded\n",
    "        TRAIN_BATCH_SIZE : int\n",
    "            specifies the batch size in the training loader\n",
    "        TEST_BATCH_SIZE : int\n",
    "            specifies the batch size in the training loader\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        train_loader, validation_loader, test_loader with the specified batch sizes \n",
    "            \n",
    "        \"\"\"\n",
    "        tranformations = transforms.Compose([transforms.ToTensor()])\n",
    "        \n",
    "        mnist_training_dataset = datasets.MNIST(root=DATA_PATH+'train', train=True, download=True, transform=tranformations)\n",
    "        mnist_testing_dataset = datasets.MNIST(root=DATA_PATH+'test', train=False, download=True, transform=tranformations)\n",
    "        \n",
    "        training_dataset, validation_dataset = random_split(mnist_training_dataset, [int(0.8*len(mnist_training_dataset)), int(0.2*len(mnist_training_dataset))])\n",
    "        \n",
    "        train_loader = DataLoader(training_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "        validation_loader = DataLoader(training_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=False)\n",
    "        test_loader = DataLoader(mnist_testing_dataset, batch_size=TEST_BATCH_SIZE, shuffle=True)\n",
    "        \n",
    "        return train_loader, validation_loader, test_loader\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Unable to get data due to ', e)\n",
    "        \n",
    "    \n",
    "def visualize_specific_predictions(specific_predictions):\n",
    "    target_values = []\n",
    "    predicted_values = []\n",
    "    figure = plt.figure(figsize=(20, 8))\n",
    "    columns = 4\n",
    "    rows = 3\n",
    "    axs = []\n",
    "    for index, (images, attacks, targets, w_preds_before_attack, w_preds_after_attack) in list(enumerate(specific_predictions))[:12]:\n",
    "        with warnings.catch_warnings(record=True):\n",
    "            axs.append( figure.add_subplot(rows, columns, index+1) )\n",
    "            axs[-1].set_title(f'Correct: {targets}, Predicted Before: {w_preds_before_attack}, Predicted After: {w_preds_after_attack}', fontsize=10)\n",
    "            axs[-1].axis(\"off\")\n",
    "            plt.imshow(images.cpu().detach().numpy().reshape(28, 28),cmap=\"gray\")\n",
    "            plt.imshow(attacks.cpu().detach().numpy().reshape(28, 28),cmap=\"gray\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n1pcmKkyjT7y"
   },
   "source": [
    "# 1. Attacking an ML-model (30 points) \n",
    "\n",
    "In this section, we implement an attack ourselves. First, however, you need a model you can attack. Feel free to choose the DNN/ConvNN from task 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QaJv_d_Dp7OM"
   },
   "source": [
    "## 1.1: Setting up the model and data (4 Points)\n",
    "\n",
    "Load the MNIST data, as done in task 1. \n",
    "\n",
    "Re-use the model from task 1 here and train it until it achieves reasonable accuracy (>92%).\n",
    "\n",
    "If you have the saved checkpoint from task 1, you can load it directly. But please compute here the test accuracy using this checkpoint.  \n",
    "\n",
    "**Hint:** In order to save computation time for the rest of exercise, you might consider having a relatively small model here.\n",
    "\n",
    "**Hint**: You might want to save the trained model to save time later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Loading data\n",
    "\n",
    "DATA_PATH = './data/'\n",
    "TRAIN_BATCH_SIZE, TEST_BATCH_SIZE = 64, 64\n",
    "train_loader, validation_loader, test_loader = _get_data(DATA_PATH, TRAIN_BATCH_SIZE, TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Defining model\n",
    "\n",
    "class CNN_Network(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        \"\"\"\n",
    "        This class is created to specify the Convolutional Neural Network on which MNIST dataset is trained on, \n",
    "        validated and later tested. \n",
    "        It consist of one input layer, one output layer can consist of multiple hidden layers all of which is \n",
    "        specified by the user as provided through model_paramaters\n",
    "        Size of the kernel, stride and padding can also be adjusted by the user as provided through model_paramaters\n",
    "        ...\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model_params : dictionary\n",
    "            provides the model with the required input size, hidden layers and output size\n",
    "            \n",
    "            model_params = {\n",
    "            'INPUT_SIZE' : int,\n",
    "            'HIDDEN_LAYERS' : list(int),\n",
    "            'OUTPUT_SIZE' : int,\n",
    "            'KERNEL' : int,\n",
    "            'STRIDE' : int,\n",
    "            'PADDING' : int\n",
    "        }\n",
    "        \"\"\"\n",
    "        try:\n",
    "            super(CNN_Network, self).__init__()\n",
    "            \n",
    "            layers = []\n",
    "            \n",
    "            for input_channel, out_channel in zip([model_params['INPUT_SIZE']] + model_params['HIDDEN_LAYERS'][:-1], \n",
    "                                                     model_params['HIDDEN_LAYERS'][:len(model_params['HIDDEN_LAYERS'])]):\n",
    "                layers.append(nn.Conv2d(input_channel, out_channel, model_params['KERNEL'], model_params['STRIDE'], model_params['PADDING'], bias=True))\n",
    "                layers.append(nn.MaxPool2d(2, 2))\n",
    "                layers.append(nn.ReLU())\n",
    "            layers.append(nn.Flatten(1))      \n",
    "            layers.append(nn.Linear(model_params['HIDDEN_LAYERS'][-1], model_params['OUTPUT_SIZE'], bias=True))\n",
    "\n",
    "            self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print('initializing failed due to ', e)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            return self.layers(x)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print('forward pass failed due to ', e)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. initializing the pre-trained model from assignment 1\n",
    "model_params = {\n",
    "        'INPUT_SIZE' : 1,\n",
    "        'HIDDEN_LAYERS' : [160, 100, 64, 10],\n",
    "        'OUTPUT_SIZE' : 10,\n",
    "        'KERNEL' : 3,\n",
    "        'STRIDE' : 1,\n",
    "        'PADDING' : 1\n",
    "}\n",
    "  \n",
    "undefended_model = CNN_Network(model_params).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading checkpoint and evaluating on test set\n",
    "def _test_model(model, test_loader, BEST_MODEL):\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(BEST_MODEL))\n",
    "        with torch.no_grad():\n",
    "            correct_predictions = []\n",
    "            testing_acc_scores = []\n",
    "            wrong_predictions = []\n",
    "            all_targets = []\n",
    "            all_preds = []\n",
    "\n",
    "\n",
    "            for images, targets in iter(test_loader):\n",
    "                images = images.to(device)\n",
    "                targets = targets.to(device)\n",
    "                outputs = model(images)\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_indicies = (preds == targets).nonzero(as_tuple=True)[0]\n",
    "                c_images = images[correct_indicies]\n",
    "                c_targets = targets[correct_indicies]\n",
    "                c_correct_preds = preds[correct_indicies]\n",
    "                testing_acc_scores.append(len(correct_indicies)/targets.shape[0])\n",
    "\n",
    "                wrong_indicies = (preds != targets).nonzero(as_tuple=True)[0]\n",
    "                w_images = images[wrong_indicies]\n",
    "                w_targets = targets[wrong_indicies]\n",
    "                w_wrong_preds = preds[wrong_indicies]\n",
    "            \n",
    "                correct_predictions += zip(c_images, c_targets, c_correct_preds)\n",
    "                wrong_predictions += zip(w_images, w_targets, w_wrong_preds)\n",
    "                all_targets+= zip(targets.cpu().numpy())\n",
    "                all_preds+= zip(preds.cpu().numpy())\n",
    "\n",
    "            return (sum(testing_acc_scores)/len(testing_acc_scores))*100, correct_predictions, wrong_predictions, all_targets, all_preds\n",
    "        \n",
    "    except Exception as e:\n",
    "            print('Error occured in testing the model = ', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_accuracy, \n",
    " correct_predictions, \n",
    " wrong_predictions, \n",
    " all_targets, all_preds) = _test_model(undefended_model, test_loader, \n",
    "                                       BEST_MODEL='Accuracy_99.8875_batchsize_64_lr_0.001.ckpt' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our test_accuracy is 99.19386942675159\n"
     ]
    }
   ],
   "source": [
    "print(f'Our test_accuracy is {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       980\n",
      "           1       0.99      1.00      1.00      1135\n",
      "           2       0.99      0.99      0.99      1032\n",
      "           3       1.00      0.99      0.99      1010\n",
      "           4       0.99      0.99      0.99       982\n",
      "           5       1.00      0.99      0.99       892\n",
      "           6       0.99      0.99      0.99       958\n",
      "           7       0.99      0.99      0.99      1028\n",
      "           8       0.99      0.99      0.99       974\n",
      "           9       0.99      0.99      0.99      1009\n",
      "\n",
      "    accuracy                           0.99     10000\n",
      "   macro avg       0.99      0.99      0.99     10000\n",
      "weighted avg       0.99      0.99      0.99     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(all_targets, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DEQrdyLHsUIu"
   },
   "source": [
    "## 1.2: Implementing the FGSM attack (7 Points)\n",
    "\n",
    "We now want to attack the model trained in the previous step. We will start with the FGSM attack as a simple example. \n",
    "\n",
    "Please implement the FGSM attack mentioned in the lecture. \n",
    "\n",
    "More details: https://arxiv.org/pdf/1412.6572.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gcVZnUNbRKOz"
   },
   "outputs": [],
   "source": [
    "def fgsm_attack(images, epsilon, clip_min, clip_max):\n",
    " \n",
    "    attack_images = images + epsilon*images.grad.sign()\n",
    "    attack_images = torch.clamp(attack_images, clip_min, clip_max)\n",
    "\n",
    "    return attack_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RNpI3oUoO1wE",
    "tags": []
   },
   "source": [
    "## 1.3: Adversarial sample set (7 Points)\n",
    "\n",
    "* Please generate a dataset containing at least 1,000 adversarial examples using FGSM.\n",
    "\n",
    "* Please vary the perturbation budget (3 variants) and generate 1,000 adversarial examples for each. \n",
    "    * **Hint**: you can choose epsilons within, e.g., = [.05, .1, .15, .2, .25, .3],  using MNIST pixel values in the interval       [0, 1]\n",
    "\n",
    "* Compute the accuracy of each attack set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_BATCH_SIZE = 1000\n",
    "_, _, test_loader = _get_data(DATA_PATH, TRAIN_BATCH_SIZE, TEST_BATCH_SIZE)\n",
    "\n",
    "fgsm_params = {'epsilons':  [0, .05, .1, .15, .2, .25, .3, .35],\n",
    "               'clip_min': 0.,\n",
    "               'clip_max': 1.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_and_attack(model, test_loader, fgsm_params):\n",
    "    try:\n",
    "        loss_func = nn.CrossEntropyLoss().to(device)\n",
    "        predictions = []\n",
    "        acc_scores = []\n",
    "        mse_scores = []\n",
    "        psnr_scores = []\n",
    "        ssim_scores = []\n",
    "        uqi_scores = []\n",
    "        sam_scores = []\n",
    "        vifp_scores = []\n",
    "\n",
    "        for epsilon in tqdm(fgsm_params['epsilons']):\n",
    "            \n",
    "            (images, targets) = next(iter(test_loader))\n",
    "            images, targets = images.to(device), targets.to(device)  \n",
    "            images.requires_grad = True \n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, _preds_before_attack = torch.max(outputs,1)\n",
    "            loss = loss_func(outputs, targets)\n",
    "            undefended_model.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            attack_images = fgsm_attack(images, epsilon, fgsm_params['clip_min'], fgsm_params['clip_max'])\n",
    "            _, _preds_after_attack = torch.max(model(attack_images),1)\n",
    "            indicies = (_preds_after_attack != targets).nonzero(as_tuple=True)[0]\n",
    "            predictions.append(zip(images[indicies], attack_images[indicies], targets[indicies], _preds_before_attack[indicies], _preds_after_attack[indicies]))\n",
    "            \n",
    "            acc_scores.append(len((_preds_after_attack == targets).nonzero(as_tuple=True)[0])/targets.shape[0]*100)\n",
    "            mse_scores.append(mse(images.cpu().detach().numpy(),attack_images.cpu().detach().numpy()))\n",
    "            psnr_scores.append(psnr(images.cpu().detach().numpy(),attack_images.cpu().detach().numpy(), MAX=1))\n",
    "            # psnrb_scores.append(psnrb(images.cpu().detach().numpy(),attack_images.cpu().detach().numpy()))\n",
    "            ssim_scores.append(ssim(attack_images.view(1000, -1).detach().numpy(),images.view(1000, -1).detach().numpy(), MAX=1))\n",
    "\n",
    "            uqi_scores.append(uqi(attack_images.view(1000, -1).detach().numpy(),images.view(1000, -1).detach().numpy()))\n",
    "            sam_scores.append(sam(attack_images.view(1000, -1).detach().numpy(),images.view(1000, -1).detach().numpy()))\n",
    "            vifp_scores.append(vifp(attack_images.view(1000, -1).detach().numpy(),images.view(1000, -1).detach().numpy()))\n",
    "            print(f\"Epsilon: {epsilon}\\tTest Accuracy = {acc_scores[-1]} % \\tMSE = {mse_scores[-1]} \\tPSNR = {psnr_scores[-1]} \\tVIFP = {vifp_scores[-1]}\")\n",
    "        \n",
    "        variants = pd.DataFrame({'Epsilon': fgsm_params['epsilons'], 'Accuracy': acc_scores, 'MSE': mse_scores, 'PSNR': psnr_scores, 'UQI': uqi_scores\n",
    "                                , 'SAM': sam_scores, 'VIFP': vifp_scores, 'SSIM': ssim_scores})\n",
    "        return variants, predictions\n",
    "    \n",
    "    except Exception as e:\n",
    "            print('Error occured in _evaluate_and_attack method = ', e)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occured in _evaluate_and_attack method =  can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10536/4015350330.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvariants\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_evaluate_and_attack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mundefended_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfgsm_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "variants, predictions = _evaluate_and_attack(undefended_model, test_loader, fgsm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(rc={\"figure.figsize\":(15, 8)}) \n",
    "sns.set_style(\"whitegrid\")\n",
    "clrs = ['grey' if (x > min(variants['Accuracy'])) else 'red' for x in variants['Accuracy']]\n",
    "\n",
    "sns.barplot(x= variants['Epsilon'], y=  variants['Accuracy'], palette=clrs)\n",
    "\n",
    "plt.xticks(fontsize=14)  \n",
    "plt.title('Decrease of Accuracy with the increase in Epsilon Values', fontsize=20)\n",
    "plt.xlabel('Epsilon Values', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.grid(alpha = 0.3, linestyle = '--', linewidth = 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moreover, it has been observed that similarity metrics can also be used to highlight the presence of an adversarial attack in an image when compared with its benign counterpart. Thus, these scores can be a measure to quantify the amount of perturbations brought in by these attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgsm_selected_sets = variants[['Epsilon', 'Accuracy']].sort_values(by='Accuracy', ascending=True).head(3)\n",
    "fgsm_selected_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ex3qQp3JolD1"
   },
   "source": [
    "## 1.4: Visualizing the results (7 Points)\n",
    "\n",
    "* Please chose one sample for each class (for example the first when iterating the test data) and plot the (ten) adversarial examples as well as the predicted label (before and after the attack)\n",
    "\n",
    "* Please repeat the visualization for the three sets you have created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eGkp0B0PO1wJ"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "\n",
    "# template code (Please feel free to change this)\n",
    "# (each column corresponds to one attack method)\n",
    "# col_titles = ['Ori','FGSM','Method 1', 'Method 2'] \n",
    "# nsamples = 10\n",
    "# nrows = nsamples\n",
    "# ncols = len(col_titles)\n",
    "\n",
    "# fig, axes = plt.subplots(nrows,ncols,figsize=(8,12))  # create the figure with subplots\n",
    "# [ax.set_axis_off() for ax in axes.ravel()]  # remove the axis\n",
    "\n",
    "# for ax, col in zip(axes[0], col_titles): # set up the title for each column\n",
    "#     ax.set_title(col,fontdict={'fontsize':18,'color':'b'})\n",
    "\n",
    "# for i in range(nsamples):\n",
    "#     axes[i,0].imshow(images_ori[i])\n",
    "#     axes[i,1].imshow(adv_FGSM[i])\n",
    "#     axes[i,2].imshow(adv_Method1[i])\n",
    "#     axes[i,3].imshow(adv_Method2[i])\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_specific_predictions(specific_predictions):\n",
    "    target_values = []\n",
    "    predicted_values = []\n",
    "    figure = plt.figure(figsize=(20, 8))\n",
    "    columns = 4\n",
    "    rows = 3\n",
    "    axs = []\n",
    "    for index, (images, attacks, targets, w_preds_before_attack, w_preds_after_attack) in list(enumerate(specific_predictions))[:12]:\n",
    "        with warnings.catch_warnings(record=True):\n",
    "            axs.append( figure.add_subplot(rows, columns, index+1) )\n",
    "            axs[-1].set_title(f'Correct: {targets}, Predicted Before: {w_preds_before_attack}, Predicted After: {w_preds_after_attack}', fontsize=10)\n",
    "            axs[-1].axis(\"off\")\n",
    "            plt.imshow(images.cpu().detach().numpy().reshape(28, 28),cmap=\"gray\")\n",
    "            plt.imshow(attacks.cpu().detach().numpy().reshape(28, 28),cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_specific_predictions(predictions[variants.query('Epsilon==0.35').index.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[variants.query('Epsilon==0.35').index.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_specific_predictions(predictions[variants.query('Epsilon==0.3').index.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_specific_predictions(predictions[variants.query('Epsilon==0.25').index.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5: Analyzing the results (5 Points)\n",
    "\n",
    "Please write a brief summary of your findings.  \n",
    "\n",
    "* Does the attack always succeed (the model makes wrong prediction on the adversarial sample)? What is the relationship between the attack success rate and the perturbation budget?\n",
    "* How about the computation cost of the attack? (you can report the time in second) \n",
    "* Does the attack require white-box access to the model?\n",
    "* Feel free to report your results via tables or figures, and mention any other interesting observations \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers go here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KJUmrv5Bymij"
   },
   "source": [
    "# 2. Defending an ML model (35 points) \n",
    "\n",
    "So far, we have focused on attacking an ML model. In this section, we want you to defend your model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0gHUFK6Mymik",
    "tags": []
   },
   "source": [
    "## 2.1: Implementing the adversarial training defense (20 Points)\n",
    "\n",
    "* We would like to ask you to implement the adversarial training defense (https://arxiv.org/pdf/1412.6572.pdf) mentioned in the lecture. \n",
    "\n",
    "* You can use the **FGSM adversarial training** method (i.e., train on FGSM examples). \n",
    "\n",
    "* You can also check the adversarial training implementation in other papers, e.g., http://proceedings.mlr.press/v97/pang19a/pang19a.pdf \n",
    "\n",
    "* Choose a certain **maximum perturbation budget** during training that is in the middle of the range you have experimented with before. \n",
    "\n",
    "* We do not require the defense to work perfectly - but what we want you to understand is why it works or why it does not work.\n",
    "\n",
    "**Hint:** You can save the checkpoint of the defended model as we would need it to for the third part of this exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _standard_training(loader, model):\n",
    "    \"\"\"Standard training/evaluation epoch over the dataset\"\"\"\n",
    "    try:\n",
    "        model.train()\n",
    "        train_loss_scores = []\n",
    "        training_acc_scores = []\n",
    "        \n",
    "        for batch_index, (images, targets) in enumerate(loader):\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "                \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            train_loss_scores.append(loss.item())\n",
    "                \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_predictions = (preds==targets).sum().item()\n",
    "            training_acc_scores.append(correct_predictions/targets.shape[0])\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "            # if (batch_index+1) % 100 == 0:\n",
    "            #     print(f\"Epoch : [{epoch+1}/{training_params['NUM_EPOCHS']}] | Step : [{batch_index+1}/{len(train_loader)}] | Loss : {loss.item()} \")\n",
    "            \n",
    "        # train_loss_history.append((sum(train_loss_scores)/len(train_loss_scores)))\n",
    "        # training_acc_history.append((sum(training_acc_scores)/len(training_acc_scores))*100)      \n",
    "        # print(f'Epoch : {epoch+1} | Loss : {train_loss_history[-1]} | Training Accuracy : {training_acc_history[-1]}%')\n",
    "\n",
    "        return training_acc_scores, train_loss_scores\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Error occured in standard training method = ', e)\n",
    "\n",
    "\n",
    "def _adversarial_training_defense(loader, model, fgsm_params):\n",
    "    \"\"\"Adversarial training/evaluation epoch over the dataset\"\"\"\n",
    "    try:\n",
    "        model.train()\n",
    "        train_loss_scores = []\n",
    "        training_acc_scores = []\n",
    "        \n",
    "        for batch_index, (images, targets) in enumerate(loader):\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            images.requires_grad = True\n",
    "            \n",
    "            # delta = fgsm_attack(images, fgsm_params['epsilon'], fgsm_params['clip_min'], fgsm_params['clip_max'])\n",
    "            outputs = model(images+0)\n",
    "            loss = criterion(outputs, targets)\n",
    "            train_loss_scores.append(loss.item())\n",
    "                \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_predictions = (preds==targets).sum().item()\n",
    "            training_acc_scores.append(correct_predictions/targets.shape[0])\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "            # if (batch_index+1) % 100 == 0:\n",
    "            #     print(f\"Epoch : [{epoch+1}/{training_params['NUM_EPOCHS']}] | Step : [{batch_index+1}/{len(train_loader)}] | Loss : {loss.item()} \")\n",
    "            \n",
    "        # train_loss_history.append((sum(train_loss_scores)/len(train_loss_scores)))\n",
    "        # training_acc_history.append((sum(training_acc_scores)/len(training_acc_scores))*100)      \n",
    "        # print(f'Epoch : {epoch+1} | Loss : {train_loss_history[-1]} | Training Accuracy : {training_acc_history[-1]}%')\n",
    "\n",
    "        return training_acc_scores, train_loss_scores\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Error occured in adversarial defense method = ', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network structure is: <bound method Module.parameters of CNN_Network(\n",
      "  (layers): Sequential(\n",
      "    (0): Conv2d(1, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(160, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(100, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): ReLU()\n",
      "    (9): Conv2d(64, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): ReLU()\n",
      "    (12): Flatten(start_dim=1, end_dim=-1)\n",
      "    (13): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      ")>\n",
      "Total number of parameters: 209244\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.984375, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 0.984375, 0.984375, 1.0, 1.0, 0.984375, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 0.984375, 1.0, 1.0, 0.984375, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 0.984375, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 0.984375, 0.984375, 0.984375, 1.0, 1.0, 1.0, 0.96875, 1.0, 0.984375, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 0.96875, 0.984375, 0.984375, 0.984375, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.984375, 1.0, 0.984375, 0.984375, 1.0, 0.984375, 1.0, 1.0, 0.984375, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 0.984375, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.953125, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 0.96875, 0.984375, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 0.984375, 1.0, 0.984375, 1.0, 0.984375, 1.0, 1.0, 1.0, 0.984375, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.984375, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 0.984375, 0.984375, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 0.984375, 0.984375, 1.0, 1.0, 0.984375, 1.0, 0.984375, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 0.984375, 0.96875, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.984375, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 0.984375, 0.984375, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 0.96875, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 0.984375, 1.0, 0.96875, 0.984375, 1.0, 1.0, 0.984375, 0.984375, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 0.984375, 0.984375, 1.0, 1.0, 0.984375, 0.984375, 0.984375, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 0.984375, 0.984375, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 0.984375, 1.0, 0.96875, 0.96875, 1.0, 1.0, 0.984375, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.984375, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.984375, 0.984375, 1.0, 0.984375, 1.0, 0.984375, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 0.984375, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 0.984375, 0.984375, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.984375, 1.0, 1.0, 0.984375, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.96875, 0.984375, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 0.984375, 1.0, 1.0, 0.984375, 1.0] [0.993, 0.989, 0.991, 0.988, 0.988, 0.996, 0.996, 0.992, 0.991, 0.998] [0.991, 0.991, 0.99, 0.993, 0.995, 0.994, 0.996, 0.995, 0.993, 0.993]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.984375, 0.984375, 0.984375, 1.0, 0.984375, 0.984375, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 0.984375, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.984375, 0.984375, 1.0, 0.953125, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 0.984375, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.984375, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 0.984375, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 0.984375, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0] [0.994, 0.988, 0.997, 0.994, 0.991, 0.995, 0.994, 0.996, 0.992, 0.993] [0.992, 0.998, 0.993, 0.995, 0.994, 0.991, 0.997, 0.998, 0.996, 0.993]\n"
     ]
    }
   ],
   "source": [
    "training_params = {\n",
    "        'TRAIN_BATCH_SIZE' : 32,\n",
    "        'TEST_BATCH_SIZE' : 1000,\n",
    "        'LEARNING_RATE' : 0.001,\n",
    "        'OPTIMIZER': optim.Adam,\n",
    "        'NUM_EPOCHS' : 2\n",
    "}\n",
    "\n",
    "fgsm_params = {'epsilon':  .35,\n",
    "               'clip_min': 0.,\n",
    "               'clip_max': 1.}\n",
    "    \n",
    "    \n",
    "model = CNN_Network(model_params).to(device)\n",
    "model.load_state_dict(torch.load('Accuracy_99.8875_batchsize_64_lr_0.001.ckpt'))\n",
    "print(f'Network structure is: {model.parameters}')\n",
    "print(f'Total number of parameters: {sum(p.numel() for p in model.parameters())}')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = training_params['OPTIMIZER'](model.parameters(), lr=training_params['LEARNING_RATE'])\n",
    "\n",
    "for t in range(2):\n",
    "    train_err, train_loss = _adversarial_training_defense(train_loader, model, fgsm_params)\n",
    "    test_err, test_loss = _standard_training(test_loader, model)\n",
    "    adv_err, adv_loss = _adversarial_training_defense(test_loader, model, fgsm_params)\n",
    "    print(train_err, test_err, adv_err)\n",
    "torch.save(model.state_dict(), \"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DD0UalSeymim"
   },
   "outputs": [],
   "source": [
    "\n",
    "def _adversarial_training_defense(model, train_loader, validation_loader, attack, criterion, optimizer, training_params, set_device, tuning=False):\n",
    "    try:\n",
    "        best_accuracy = 0\n",
    "        train_loss_history = []\n",
    "        validation_loss_history = []\n",
    "        training_acc_history = []\n",
    "        validation_acc_history = []\n",
    "        \n",
    "        for epoch in range(0, training_params['NUM_EPOCHS']):\n",
    "            model.train()\n",
    "            train_loss_scores = []\n",
    "            training_acc_scores = []\n",
    "            correct_predictions = 0\n",
    "            \n",
    "            for batch_index, (images, targets) in enumerate(train_loader):\n",
    "                images, targets = images.to(set_device), targets.to(set_device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, targets)\n",
    "                train_loss_scores.append(loss.item())\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_predictions = (preds==targets).sum().item()\n",
    "                training_acc_scores.append(correct_predictions/targets.shape[0])\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                if not tuning:\n",
    "                    if (batch_index+1) % 100 == 0:\n",
    "                        print(f\"Epoch : [{epoch+1}/{training_params['NUM_EPOCHS']}] | Step : [{batch_index+1}/{len(train_loader)}] | Loss : {loss.item()} \")\n",
    "            \n",
    "            train_loss_history.append((sum(train_loss_scores)/len(train_loss_scores)))\n",
    "            training_acc_history.append((sum(training_acc_scores)/len(training_acc_scores))*100)      \n",
    "            print(f'Epoch : {epoch+1} | Loss : {train_loss_history[-1]} | Training Accuracy : {training_acc_history[-1]}%')\n",
    "       \n",
    "            return train_loss_history, training_acc_history\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Error occured in training method = ', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_loss, validation_loss, train_acc, validation_acc = _network_training(model, train_loader, validation_loader, criterion, optimizer, training_params, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: Evaluation (10 Points)\n",
    "\n",
    "* Craft adversarial examples using the **defended** model. This entails at least 1,000 examples crafted via FGSM. \n",
    "    * Create one set using a budget that is **less than (within)** the one used in training.\n",
    "    * Create another set using a budget that is **higher than** the one used in training. \n",
    "    * You can use two values of epsilons from question 1.3 \n",
    "    \n",
    "* Evaluate the **defended** model on these two adversarial examples sets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "\n",
    "print('Accuracy on the lower-budget adversarial samples (FGSM) %.2f'%acc_FGSM1)\n",
    "print('Accuracy on the lower-budget adversarial samples (FGSM) after defense %.2f'%acc_FGSM_defend1)\n",
    "\n",
    "print('Accuracy on the higher-budget adversarial samples (FGSM) %.2f'%acc_FGSM2)\n",
    "print('Accuracy on the higher-budget adversarial samples (FGSM) after defense %.2f'%acc_FGSM_defend2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Discussion (5 points)\n",
    "\n",
    "* How successful was the defense against the attack compared to the undefended model? How do you interpret the difference?\n",
    "* How did the two sets differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers go here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: I-FGSM attack (35 points) \n",
    "\n",
    "* FGSM is one of the simplest and earliest attacks. Since then, many more advanced attacks have been proposed. \n",
    "* One of them is the Iterative-FGSM (https://arxiv.org/pdf/1607.02533.pdf), where the attack is repeated multiple times.\n",
    "* In this part, we ask you to please implement the iterative FGSM attack. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1: Implementing the I-FGSM attack (10 Points)\n",
    "\n",
    "**Hints**: \n",
    "\n",
    "* Your code should have an attack loop. At each step, the FGSM attack that you have implemented before is computed using a small step.\n",
    "* After each step, you should perform a per-pixel clipping to make sure the image is in the allowed range, and that the perturbation is within budget.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2: Attack the undefended model (5 Points)\n",
    "\n",
    "* We will first attack the **undefended model** (i.e., without adversarial training).\n",
    "\n",
    "* Choose one perturbation budget from Question **1.3** for comparison. \n",
    "\n",
    "    * Hint: A simple way to choose the small step is to divide the total budget by the number of steps (e.g., 10).\n",
    "\n",
    "* Please generate 1000 adversarial examples using the **undefended** model and the **I-FGSM** you implemented. \n",
    "\n",
    "* Please compute the accuracy of the adversarial set on the **undefended** model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1: Findings and comparison with FGSM (8 points)\n",
    "\n",
    "* Please report your findings. How successful was the attack? \n",
    "\n",
    "* What do you expect when increasing the number of steps? (you can experiment with different parameters of the attack and report your findings) \n",
    "\n",
    "* Compare with the basic FGSM. Using the same perturbation budget and using the same model, which attack is more successful? Why do you think this is the case? What about the computation time?\n",
    "\n",
    "* Feel free to report any interesting observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers go here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3: Attack the defended model (5 poinst) \n",
    "\n",
    "* In the previous question, we attacked the **undefended model**. \n",
    "\n",
    "* Now, we want to explore how successful the previous implemented defense (FGSM adversarial training) is againts this new attack. (we will not implement a new defense here, we will be reusing your previous checkpoint of the **defended model**)\n",
    "\n",
    "\n",
    "* Use the **defended model** to create one set of adversarial examples. Use a perturbation budget from Question **2.2** for comparison.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1: Discussion (7 points) \n",
    "* Please report your results. How successful was the attack on the defended model? \n",
    "* Compare it with the success of the FGSM attack on the defended model. What do you observe? How do you interpret the difference? \n",
    "* How do you think you can improve the defense against I-FGSM attack?\n",
    "\n",
    "\n",
    "* Feel free to state any interesting findings you encountered during this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers go here**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project_3_Template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
