{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!mkdir data\n%cd data\n!wget https://nextcloud.mpi-klsb.mpg.de/index.php/s/pJrRGzm2So2PMZm/download -O train.tar.gz \n!tar xzf train.tar.gz\n!wget https://nextcloud.mpi-klsb.mpg.de/index.php/s/zN3yeWzQB3i5WqE/download -O test.tar.gz \n!tar xzf test.tar.gz\n%cd ..","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ML in Cybersecurity: Task 3\n\n## Team\n  * **Team name**:  *R2D2C3P0BB8*\n  * **Members**:  <br/> **Navdeeppal Singh (s8nlsing@stud.uni-saarland.de)** <br/> **Shahrukh Khan (shkh00001@stud.uni-saarland.de)** <br/> **Mahnoor Shahid (mash00001@stud.uni-saarland.de)**\n\n\n## Logistics\n  * **Due date**: 9th December 2021, 23:59:59\n  * Email the completed notebook to: `mlcysec_ws2022_staff@lists.cispa.saarland`\n  * Complete this in **teams of 3**\n  * Feel free to use the forum to discuss.\n  \n## Timeline\n  * 26-Nov-2021: hand-out\n  * **09-Dec-2021**: Email completed notebook\n  \n  \n## About this Project\nIn this project, you will explore an application of ML to a popular task in cybersecurity: malware classification.\nYou will be presented with precomputed behaviour analysis reports of thousands of program binaries, many of which are malwares.\nYour goal is to train a malware detector using this behavioural reports.\n\n\n## A Note on Grading\nThe grading for this project will depend on:\n 1. Vectorizing Inputs\n   * Obtaining a reasonable vectorized representations of the input data (a file containing a sequence of system calls)\n   * Understanding the influence these representations have on your model\n 1. Classification Model  \n   * Following a clear ML pipeline\n   * Obtaining reasonable performances (>60\\%) on held-out test set\n   * Choice of evaluation metric\n   * Visualizing loss/accuracy curves\n 1. Analysis\n   * Which methods (input representations/ML models) work better than the rest and why?\n   * Which hyper-parameters and design-choices were important in each of your methods?\n   * Quantifying influence of these hyper-parameters on loss and/or validation accuracies\n   * Trade-offs between methods, hyper-parameters, design-choices\n   * Anything else you find interesting (this part is open-ended)\n\n\n## Grading Details\n * 40 points: Vectorizing input data (each input = behaviour analysis file in our case)\n * 40 points: Training a classification model\n * 15 points: Analysis/Discussion\n * 5 points: Clean code\n \n## Filling-in the Notebook\nYou'll be submitting this very notebook that is filled-in with your code and analysis. Make sure you submit one that has been previously executed in-order. (So that results/graphs are already visible upon opening it). \n\nThe notebook you submit **should compile** (or should be self-contained and sufficiently commented). Check tutorial 1 on how to set up the Python3 environment.\n\n\n**The notebook is your project report. So, to make the report readable, omit code for techniques/models/things that did not work. You can use the final summary to provide a report about these.**\n\nIt is extremely important that you **do not** re-order the existing sections. Apart from that, the code blocks that you need to fill-in are given by:\n```\n#\n#\n# ------- Your Code -------\n#\n#\n```\nFeel free to break this into multiple-cells. It's even better if you interleave explanations and code-blocks so that the entire notebook forms a readable \"story\".\n\n\n## Code of Honor\nWe encourage discussing ideas and concepts with other students to help you learn and better understand the course content. However, the work you submit and present **must be original** and demonstrate your effort in solving the presented problems. **We will not tolerate** blatantly using existing solutions (such as from the internet), improper collaboration (e.g., sharing code or experimental data between groups) and plagiarism. If the honor code is not met, no points will be awarded.\n\n \n ## Versions\n  * v1.1: Updated deadline\n  * v1.0: Initial notebook\n  \n  ---","metadata":{"id":"qKo3S_fOsMim"}},{"cell_type":"code","source":"import time \n \nimport numpy as np \nimport matplotlib.pyplot as plt \n\nimport json \nimport time \nimport pickle \nimport sys \nimport csv \nimport os \nimport os.path as osp \nimport shutil \nimport pathlib\nfrom pathlib import Path\n\nfrom IPython.display import display, HTML\n \n%matplotlib inline \nplt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots \nplt.rcParams['image.interpolation'] = 'nearest' \nplt.rcParams['image.cmap'] = 'gray' \n \n# for auto-reloading external modules \n# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython \n%load_ext autoreload\n%autoreload 2","metadata":{"id":"uyCiLbXbsMiq","execution":{"iopub.status.busy":"2021-11-29T06:05:40.274892Z","iopub.execute_input":"2021-11-29T06:05:40.275829Z","iopub.status.idle":"2021-11-29T06:05:40.344178Z","shell.execute_reply.started":"2021-11-29T06:05:40.275743Z","shell.execute_reply":"2021-11-29T06:05:40.343236Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Some suggestions of our libraries that might be helpful for this project\nfrom collections import Counter          # an even easier way to count\nfrom multiprocessing import Pool         # for multiprocessing\nfrom tqdm import tqdm                    # fancy progress bars\n\n# Load other libraries here.\nfrom sklearn.metrics import recall_score\n# Keep it minimal! We should be easily able to reproduce your code.\n\n# We preload pytorch as an example\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset","metadata":{"id":"7dHogdxRsMis","execution":{"iopub.status.busy":"2021-11-29T06:05:40.346340Z","iopub.execute_input":"2021-11-29T06:05:40.346688Z","iopub.status.idle":"2021-11-29T06:05:42.759765Z","shell.execute_reply.started":"2021-11-29T06:05:40.346647Z","shell.execute_reply":"2021-11-29T06:05:42.758834Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Setup\n\n  * Download the datasets: [train](https://nextcloud.mpi-klsb.mpg.de/index.php/s/pJrRGzm2So2PMZm) (128M) and [test](https://nextcloud.mpi-klsb.mpg.de/index.php/s/zN3yeWzQB3i5WqE) (92M)\n  * Unpack them under `./data/train` and `./data/test`\n  * Hint: you can execute shell scripts from notebooks using the `!` prefix, e.g., `! wget <url>`","metadata":{"id":"oJWkh3GUsMit"}},{"cell_type":"code","source":"# Check that you are prepared with the data\n! printf '# train examples (Should be 13682) : '; ls data/train | wc -l\n! printf '# test  examples (Should be 10000) : '; ls data/test | wc -l","metadata":{"id":"wlhs4w44sMit","execution":{"iopub.status.busy":"2021-11-29T05:18:52.207060Z","iopub.execute_input":"2021-11-29T05:18:52.207306Z","iopub.status.idle":"2021-11-29T05:18:54.068254Z","shell.execute_reply.started":"2021-11-29T05:18:52.207278Z","shell.execute_reply":"2021-11-29T05:18:54.067063Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"# train examples (Should be 13682) : 13682\n# test  examples (Should be 10000) : 10000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now that you're set, let's briefly look at the data you have been handed.\nEach file encodes the behavior report of a program (potentially a malware), using an encoding scheme called \"The Malware Instruction Set\" (MIST for short).\nAt this point, we highly recommend you briefly read-up Sec. 2 of the [MIST](http://www.mlsec.org/malheur/docs/mist-tr.pdf) documentation.\n\nYou will find each file named as `filename.<malwarename>`:\n```\n» ls data/train | head\n00005ecc06ae3e489042e979717bb1455f17ac9d.NothingFound\n0008e3d188483aeae0de62d8d3a1479bd63ed8c9.Basun\n000d2eea77ee037b7ef99586eb2f1433991baca9.Patched\n000d996fa8f3c83c1c5568687bb3883a543ec874.Basun\n0010f78d3ffee61101068a0722e09a98959a5f2c.Basun\n0013cd0a8febd88bfc4333e20486bd1a9816fcbf.Basun\n0014aca72eb88a7f20fce5a4e000c1f7fff4958a.Texel\n001ffc75f24a0ae63a7033a01b8152ba371f6154.Texel\n0022d6ba67d556b931e3ab26abcd7490393703c4.Basun\n0028c307a125cf0fdc97d7a1ffce118c6e560a70.Swizzor\n...\n```\nand within each file, you will see a sequence of individual systems calls monitored duing the run-time of the binary - a malware named 'Basun' in the case:\n```\n» head data/train/000d996fa8f3c83c1c5568687bb3883a543ec874.Basun\n# process 000006c8 0000066a 022c82f4 00000000 thread 0001 #\n02 01 | 000006c8 0000066a 00015000\n02 02 | 00006b2c 047c8042 000b9000\n02 02 | 00006b2c 047c8042 00108000\n02 02 | 00006b2c 047c8042 00153000\n02 02 | 00006b2c 047c8042 00091000\n02 02 | 00006b2c 047c8042 00049000\n02 02 | 00006b2c 047c8042 000aa000\n02 02 | 00006b2c 047c8042 00092000\n02 02 | 00006b2c 047c8042 00011000\n...\n```\n(**Note**: Please ignore the first line that begins with `# process ...`.)\n\nYour task in this project is to train a malware detector, which given the sequence of system calls (in the MIST-formatted file like above), predicts one of 10 classes: `{ Agent, Allaple, AutoIt, Basun, NothingFound, Patched, Swizzor, Texel, VB, Virut }`, where `NothingFound` roughly represents no malware is present.\nIn terms of machine learning terminology, your malware detector $F: X \\rightarrow Y$ should learn a mapping from the MIST-encoded behaviour report (the input $x \\in X$) to the malware class $y \\in Y$.\n\nConsequently, you will primarily tackle two challenges in this project:\n  1. \"Vectorizing\" the input data i.e., representing each input (file) as a tensor\n  1. Training an ML model\n  \n\n### Some tips:\n  * Begin with an extremely simple representation/ML model and get above chance-level classification performance\n  * Choose your evaluation metric wisely\n  * Save intermediate computations (e.g., a token to index mapping). This will avoid you parsing the entire dataset for every experiment\n  * Try using `multiprocessing.Pool` to parallelize your `for` loops","metadata":{"id":"Zh1-JHNIsMiv"}},{"cell_type":"markdown","source":"---","metadata":{"id":"p8xloR8dsMiv"}},{"cell_type":"markdown","source":"# 1. Vectorize Data","metadata":{"id":"vXnyDAdbsMiw"}},{"cell_type":"markdown","source":"## 1.a. Load Raw Data\n## => We converted list of list lines to string to save memory in order to load entire dataset","metadata":{"id":"Ia-xFIQtsMiw"}},{"cell_type":"code","source":"def load_content(filepath):\n    '''Given a filepath, returns (content, classname), where content = [list of lines in file]'''\n    ## load file content\n    file = open(filepath, \"r\")\n    file_lines = file.read()\n    ## here converted list of list lines to string to save memory in order to load entire dataset\n    lines = \"\\n\".join(file_lines.splitlines())\n    file.close()\n\n    ## extracting label\n    label = filepath.split(\".\")[-1]\n    return lines, label\n\n\ndef load_data(data_path, nworkers=10):\n    '''Returns each data sample as a tuple (x, y), x = sequence of strings (i.e., syscalls), y = malware program class'''\n    raw_data_samples = []\n    \n    file_paths = [f\"{data_path}/{filename}\" for filename in os.listdir(data_path)]\n    pool = Pool(processes=nworkers)\n \n    raw_data_samples = pool.map(load_content, file_paths)\n    return raw_data_samples","metadata":{"id":"ZScY-C2IsMiw","execution":{"iopub.status.busy":"2021-11-29T05:18:54.070499Z","iopub.execute_input":"2021-11-29T05:18:54.070929Z","iopub.status.idle":"2021-11-29T05:18:54.125093Z","shell.execute_reply.started":"2021-11-29T05:18:54.070881Z","shell.execute_reply":"2021-11-29T05:18:54.124057Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_path = './data/train'\ntest_path = './data/test'\nn_workers = 10","metadata":{"id":"7BSUdq_-sMiw","scrolled":true,"execution":{"iopub.status.busy":"2021-11-29T05:18:54.127630Z","iopub.execute_input":"2021-11-29T05:18:54.127997Z","iopub.status.idle":"2021-11-29T05:18:54.171683Z","shell.execute_reply.started":"2021-11-29T05:18:54.127953Z","shell.execute_reply":"2021-11-29T05:18:54.170594Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"project_mode = 'trainval'    # trainval, traintest, debug, eval\nnp.random.seed(123)          # To perform the same split across multiple runs\n\n## in trainval mode we use test_raw_samples variable to hold validation dataset\ntrain_raw_samples, test_raw_samples = [], []\n \nif project_mode == 'trainval':\n    print('=> Loading training data ... ')\n    train_raw_samples = load_data(Path(train_path), nworkers=n_workers)\n    # Split data into train and validation set\n    np.random.shuffle(train_raw_samples)\n    train_raw_samples, test_raw_samples = train_raw_samples[:int(len(train_raw_samples)*0.8)], train_raw_samples[int(len(train_raw_samples)*0.8):]\n\nelif project_mode == 'traintest':\n    ## loading train and test set\n    print('=> Loading training data ... ')\n    train_raw_samples = load_data(Path(train_path), nworkers=n_workers)\n    print('=> Loading testing data ... ')\n    test_raw_samples = load_data(Path(test_path), nworkers=n_workers)\n    \nelif project_mode == 'debug':\n    print('=> Loading training data ... ')\n    train_raw_samples = load_data(Path(train_path), nworkers=n_workers)\n    print('=> Loading testing data ... ')\n    test_raw_samples = load_data(Path(test_path), nworkers=n_workers)\n    # Optional, use a small subset of the training and validation data for fast debugging\n    train_set, test_set = train_raw_samples[:100], test_raw_samples[:100]\n\nelif project_mode == 'eval':\n    ## load only test set for evaluating the model\n    print('=> Loading testing data ... ')\n    test_raw_samples = load_data(Path(test_path), nworkers=n_workers)\n\nelse:\n    raise ValueError('Unrecognized mode')\n    \nprint('=> # Train samples = ', len(train_raw_samples))\nprint('=> # Test  samples = ', len(test_raw_samples))","metadata":{"id":"GG6tcSNJsMiw","execution":{"iopub.status.busy":"2021-11-29T05:18:54.172848Z","iopub.execute_input":"2021-11-29T05:18:54.173649Z","iopub.status.idle":"2021-11-29T05:20:21.432087Z","shell.execute_reply.started":"2021-11-29T05:18:54.173609Z","shell.execute_reply":"2021-11-29T05:20:21.430808Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"=> Loading training data ... \n=> # Train samples =  10945\n=> # Test  samples =  2737\n","output_type":"stream"},{"name":"stderr","text":"Process ForkPoolWorker-6:\nProcess ForkPoolWorker-9:\nProcess ForkPoolWorker-8:\nProcess ForkPoolWorker-3:\nProcess ForkPoolWorker-10:\nProcess ForkPoolWorker-2:\nProcess ForkPoolWorker-1:\nProcess ForkPoolWorker-4:\nProcess ForkPoolWorker-7:\nProcess ForkPoolWorker-5:\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n    task = get()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n    task = get()\n  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n    task = get()\n  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n    task = get()\n  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n    task = get()\n  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n    task = get()\n  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n    task = get()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n    with self._rlock:\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n    with self._rlock:\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n    with self._rlock:\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n    with self._rlock:\n  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n    task = get()\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n    with self._rlock:\n  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n    task = get()\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n    with self._rlock:\n  File \"/opt/conda/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n    with self._rlock:\n  File \"/opt/conda/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n    with self._rlock:\n  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n    task = get()\n  File \"/opt/conda/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/opt/conda/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\nKeyboardInterrupt\n  File \"/opt/conda/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/opt/conda/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n    res = self._reader.recv_bytes()\nKeyboardInterrupt\nKeyboardInterrupt\nKeyboardInterrupt\nKeyboardInterrupt\n  File \"/opt/conda/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n    buf = self._recv_bytes(maxlength)\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n    with self._rlock:\nKeyboardInterrupt\n  File \"/opt/conda/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/opt/conda/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\nKeyboardInterrupt\nKeyboardInterrupt\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 1.b. Vectorize: Setup\n\nMake one pass over the inputs to identify relevant features/tokens.\n\nSuggestion:\n  - identify tokens (e.g., unigrams, bigrams)\n  - create a token -> index (int) mapping. Note that you might have a >10K unique tokens. So, you will have to choose a suitable \"vocabulary\" size.","metadata":{"id":"GjiNiOVHsMiy"}},{"cell_type":"code","source":"# Feel free to edit anything in this block\n\ndef get_key_idx_map(counter, vocab_size, ukn_token='_ukn_'):\n    \"\"\"counter is a mapping: token -> count\n    build vectorizer using vocab_size most common elements\"\"\"\n    key_to_idx, idx_to_key = dict(), dict()\n    \n    for idx, (key, value) in tqdm(enumerate(list(train_counter.items())[:vocab_size-1])):\n        ## perform mapping for token\n        key_to_idx[key] = idx\n        idx_to_key[idx] = key\n    ## perform mapping for unk token at the end\n    key_to_idx[ukn_token] = vocab_size - 1\n    idx_to_key[vocab_size - 1] = ukn_token\n    \n    return key_to_idx, idx_to_key\n\ndef preprocess(data):\n    \"\"\"concatenating all sys calls to single string for tokenization\n    removing extraneous information such as lines with '# process', white spaces and '|' characters\"\"\"\n    for i, (X,y) in enumerate(tqdm(data)):\n        example = \"\"\n        for line in X.split(\"\\n\"):\n            ## skip lines containing '# process'\n            if \"# process\" in line:\n                continue\n            ## remove extraneous white spaces and \n            example += line.replace(\"|\",\"\").replace(\"  \", \" \").strip() + \" \"\n        example = example.strip()\n        ## assign preprocessed sample\n        data[i] = (example, y)\n    return data\n        \ndef count_words(data):\n    \"\"\"\n    count token occurences for building vocabulary later\n    \"\"\"\n    counter = {}\n    for X,y in tqdm(data):\n        counts = dict(Counter(X.split()))\n        counter = dict(counter, **counts)\n    return counter","metadata":{"id":"DAtmzL6PsMiy","execution":{"iopub.status.busy":"2021-11-29T05:20:21.436121Z","iopub.execute_input":"2021-11-29T05:20:21.436449Z","iopub.status.idle":"2021-11-29T05:20:21.542924Z","shell.execute_reply.started":"2021-11-29T05:20:21.436403Z","shell.execute_reply":"2021-11-29T05:20:21.541934Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nPreprocessing both train and test set and\nCreating token counter for building vocabulary on train set\n\"\"\"\ntrain_counter = None\nif project_mode != \"eval\":\n    train_raw_samples = preprocess(train_raw_samples)\n    train_counter = count_words(train_raw_samples)\ntest_raw_samples = preprocess(test_raw_samples)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T05:20:21.545003Z","iopub.execute_input":"2021-11-29T05:20:21.545912Z","iopub.status.idle":"2021-11-29T05:29:07.252367Z","shell.execute_reply.started":"2021-11-29T05:20:21.545867Z","shell.execute_reply":"2021-11-29T05:29:07.251027Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"100%|██████████| 10945/10945 [03:01<00:00, 60.16it/s] \n100%|██████████| 10945/10945 [05:02<00:00, 36.14it/s]\n100%|██████████| 2737/2737 [00:40<00:00, 66.99it/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"## Code for finding appropriate threshold for setting `MAX_VOCAB_SIZE`\ndef choose_vocab_size(min_frequency_threshold=10):\n    count = 0\n    for value,key in sorted([(value,key) for (key,value) in train_counter.items()], reverse=True):\n        if value > min_frequency_threshold:\n            count+=1\n    print(f\"Number of tokens are {count} for min. frequency threshold={min_frequency_threshold}\")\n#choose_vocab_size(min_frequency_threshold=10)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T05:29:07.254150Z","iopub.execute_input":"2021-11-29T05:29:07.254544Z","iopub.status.idle":"2021-11-29T05:29:07.316755Z","shell.execute_reply.started":"2021-11-29T05:29:07.254492Z","shell.execute_reply":"2021-11-29T05:29:07.315583Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"## sorting the counters wrt to count values in decending order\ntrain_counter = {key:value for value, key in  sorted([(value,key) for (key,value) in train_counter.items()], reverse=True)}","metadata":{"execution":{"iopub.status.busy":"2021-11-29T05:29:07.318205Z","iopub.execute_input":"2021-11-29T05:29:07.318553Z","iopub.status.idle":"2021-11-29T05:29:07.763037Z","shell.execute_reply.started":"2021-11-29T05:29:07.318506Z","shell.execute_reply":"2021-11-29T05:29:07.762137Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Feel free to edit anything in this block\n## By keeping a minimum count threshold of 10 we get 6316 most frequent tokens in train dataset\n## adding one to MAX_VOCAB_SIZE for _ukn_ token\nMAX_VOCAB_SIZE = 5701\n\ntoken_to_idx, idx_to_token = get_key_idx_map(train_counter, MAX_VOCAB_SIZE)\n\n# Save vocab to file\nout_path = 'application_vocab_{}.pkl'.format(MAX_VOCAB_SIZE)\nwith open(out_path, 'wb') as wf:\n    dct = {'token_to_idx': token_to_idx,\n          'idx_to_token': idx_to_token}\n    pickle.dump(dct, wf)","metadata":{"id":"LSMEAcgvsMiy","execution":{"iopub.status.busy":"2021-11-29T05:29:07.765894Z","iopub.execute_input":"2021-11-29T05:29:07.766145Z","iopub.status.idle":"2021-11-29T05:29:07.871240Z","shell.execute_reply.started":"2021-11-29T05:29:07.766114Z","shell.execute_reply":"2021-11-29T05:29:07.870319Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"5700it [00:00, 670166.87it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 1.c. Vectorize Data\n\nUse the (token $\\rightarrow$ index) mapping you created before to vectorize your data","metadata":{"id":"QUefzVjFsMiy"}},{"cell_type":"code","source":"def sample_to_idx(sample):\n    idx_sample = []\n    for token in sample.split(' '):\n        if token not in token_to_idx:\n                token = '_ukn_'\n        idx_sample.append(token_to_idx[token])\n    return idx_sample","metadata":{"id":"ffJJ0XOvsMiz","execution":{"iopub.status.busy":"2021-11-29T05:29:07.873063Z","iopub.execute_input":"2021-11-29T05:29:07.873775Z","iopub.status.idle":"2021-11-29T05:29:07.917849Z","shell.execute_reply.started":"2021-11-29T05:29:07.873724Z","shell.execute_reply":"2021-11-29T05:29:07.916690Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"## define mapping for labels\nlabel_encodings = {'Virut': 0,\n 'Swizzor': 1,\n 'Agent': 2,\n 'Patched': 3,\n 'Allaple': 4,\n 'Texel': 5,\n 'Basun': 6,\n 'AutoIt': 7,\n 'NothingFound': 8,\n 'VB': 9}","metadata":{"execution":{"iopub.status.busy":"2021-11-29T06:05:45.525585Z","iopub.execute_input":"2021-11-29T06:05:45.525880Z","iopub.status.idle":"2021-11-29T06:05:45.576784Z","shell.execute_reply.started":"2021-11-29T06:05:45.525849Z","shell.execute_reply":"2021-11-29T06:05:45.575269Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def vectorize_raw_samples(raw_samples, vocab_length, nworkers=10):\n    labels = []\n    lengths = []\n    tf_samples = [] ## term frequency vector for each sample\n    for idx, (X,y) in tqdm(enumerate(raw_samples)):\n        vectorized_sample = []\n        ## map labeks to ids\n        label = label_encodings[y]\n        ## map tokens to ids\n        X_idx = sample_to_idx(X)\n        \n        ## initializing placeholder vector with unknown tokens equivalent to max_length\n        vector_sample = [0] * vocab_length\n        ## compute counts\n        counts = dict(Counter(X_idx))\n        ## creating Count Vectors \n        for index, (key, val) in enumerate(counts.items()):\n            vector_sample[key] = val\n        sequence_length = len(X_idx)\n        ## compute term frequencies 'tf => # of times term in the doc / total words in the doc'\n        term_frequencies = np.array(vector_sample) / sequence_length\n        ## append sample to respective lists\n        labels.append(label)\n        lengths.append(sequence_length)\n        tf_samples.append(term_frequencies)\n    \n    # compute idf\n    # 1. computing BOW matrix \n    bow = np.zeros(shape=(len(raw_samples), vocab_length))\n    for i in range(len(tf_samples)):\n        for j in range(vocab_length):\n            if tf_samples[i][j] > 0:\n                bow[i,j] = 1\n    # 2, compute idf scores 'idf(t) => log( ((1 + # of docs)/ # of docs with term t + 1) + 1 ) '\n    idf = [np.log(((1+len(raw_samples))/(1+sum(bow[:, i])))+1) for i in range(vocab_length)]\n    \n    # compute tf-idf => tf * idf\n    tf = np.array(tf_samples)\n    tf_idf = np.zeros(shape=(len(raw_samples), vocab_length))\n    for i in range(vocab_length):\n        tf_idf[:, i] = tf[:, i] * idf[i]\n    return (torch.DoubleTensor(tf_idf), torch.LongTensor(labels), torch.LongTensor(lengths))","metadata":{"id":"CD5IqJmXsMiz","execution":{"iopub.status.busy":"2021-11-29T06:05:46.852521Z","iopub.execute_input":"2021-11-29T06:05:46.852798Z","iopub.status.idle":"2021-11-29T06:05:46.906492Z","shell.execute_reply.started":"2021-11-29T06:05:46.852769Z","shell.execute_reply":"2021-11-29T06:05:46.905134Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"print('=> Processing: Train')\ntrain_data = vectorize_raw_samples(train_raw_samples, vocab_length=MAX_VOCAB_SIZE)\nprint()\nprint('=> Processing: Test')\ntest_data = vectorize_raw_samples(test_raw_samples, vocab_length=MAX_VOCAB_SIZE)","metadata":{"id":"BtS10ASbsMi0","scrolled":true,"execution":{"iopub.status.busy":"2021-11-29T05:49:08.565750Z","iopub.execute_input":"2021-11-29T05:49:08.566201Z","iopub.status.idle":"2021-11-29T05:58:30.266554Z","shell.execute_reply.started":"2021-11-29T05:49:08.566155Z","shell.execute_reply":"2021-11-29T05:58:30.265652Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"=> Processing: Train\n","output_type":"stream"},{"name":"stderr","text":"10945it [06:07, 29.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n=> Processing: Test\n","output_type":"stream"},{"name":"stderr","text":"2737it [01:28, 30.93it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Suggestions: \n#\n# (a) You can use torch.utils.data.TensorDataset to represent the tensors you created previously\n# trainset = TensorDataset(train_x, train_y)\n# testset = TensorDataset(test_x, test_y)\n#\n# (b) Store your datasets to disk so that you do not need to precompute it every time\n\n\"\"\"\nStandard Pytorch Dataset class for loading datasets.\n\"\"\"\nclass MalwareDataset(Dataset):\n\n    def __init__(self, data_tensor, target_tensor, length_tensor):\n        \"\"\"\n        initializes  and populates the the length, data and target tensors, and raw texts list\n        \"\"\"\n        assert data_tensor.size(0) == target_tensor.size(0) == length_tensor.size(0)\n        self.data_tensor = data_tensor\n        self.target_tensor = target_tensor\n        self.length_tensor = length_tensor\n\n    def __getitem__(self, index):\n        \"\"\"\n        returns the tuple of data tensor, targets, lengths of sequences tensor\n        \"\"\"\n        return self.data_tensor[index], self.target_tensor[index], self.length_tensor[index]\n\n    def __len__(self):\n        \"\"\"\n        returns the length of the data tensor.\n        \"\"\"\n        return self.data_tensor.size(0)\n\n## instantiate train and test datasets\nmalware_testset = MalwareDataset(test_data[0], test_data[1], test_data[2])\nmalware_trainset = MalwareDataset(train_data[0], train_data[1], train_data[2])","metadata":{"id":"0XOHr24usMi0","execution":{"iopub.status.busy":"2021-11-29T06:06:27.356745Z","iopub.execute_input":"2021-11-29T06:06:27.357049Z","iopub.status.idle":"2021-11-29T06:06:27.411390Z","shell.execute_reply.started":"2021-11-29T06:06:27.357008Z","shell.execute_reply":"2021-11-29T06:06:27.410332Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# 2. Train Model\n\nYou will now train an ML model on the vectorized datasets you created previously.\n\n_Note_: Although we often refer to each input as a 'vector' for simplicity, each of your inputs can also be higher dimensional tensors.","metadata":{"id":"_l0tLsEVsMi0"}},{"cell_type":"markdown","source":"## 2.a. Helpers","metadata":{"id":"UInY1YzksMi0"}},{"cell_type":"code","source":"# Feel free to edit anything in this block\n## temporarily upload files to cloud for moving them around: !curl --upload-file ./train_v5700_l2000.pkl https://transfer.sh/train_v5700_l2000.pkl\n\ndef evaluate_preds(y_gt, y_pred):\n    recall = recall_score(y_gt, y_pred, average='macro')\n    return recall\n\n\ndef another_helper(question):\n    return 42\n\n\ndef save_model(model, out_path):\n    pass\n\n\ndef pickle_file(out_path, file_content):\n    with open(out_path, 'wb') as wf:\n        pickle.dump(file_content, wf)\n        \ndef unpickle_file(in_path):\n    return pickle.load(open(in_path, \"rb\"))","metadata":{"id":"VnSv0J75sMi0","execution":{"iopub.status.busy":"2021-11-29T06:05:55.820987Z","iopub.execute_input":"2021-11-29T06:05:55.821309Z","iopub.status.idle":"2021-11-29T06:05:55.874656Z","shell.execute_reply.started":"2021-11-29T06:05:55.821253Z","shell.execute_reply":"2021-11-29T06:05:55.873234Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#!curl --upload-file ./test_v5701_tfidf_vec.pkl https://transfer.sh/test_v5701_tfidf_vec.pkl\n#pickle_file('train_v5701_tfidf_vec.pkl', train_data)\n#pickle_file('test_v5701_tfidf_vec.pkl', test_data)\n\n#application_vocab_5701_count_vec\n!wget https://transfer.sh/pWihbm/test_v5701_tfidf_vec.pkl\n#!wget https://transfer.sh/WTptcx/application_vocab_5701.pkl\n!wget https://transfer.sh/zHSiyI/train_v5701_tfidf_vec.pkl\n","metadata":{"execution":{"iopub.status.busy":"2021-11-29T06:05:56.255455Z","iopub.execute_input":"2021-11-29T06:05:56.255907Z","iopub.status.idle":"2021-11-29T06:06:25.928796Z","shell.execute_reply.started":"2021-11-29T06:05:56.255863Z","shell.execute_reply":"2021-11-29T06:06:25.927488Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"--2021-11-29 06:05:57--  https://transfer.sh/pWihbm/test_v5701_tfidf_vec.pkl\nResolving transfer.sh (transfer.sh)... 144.76.136.153\nConnecting to transfer.sh (transfer.sh)|144.76.136.153|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 124873879 (119M)\nSaving to: ‘test_v5701_tfidf_vec.pkl’\n\ntest_v5701_tfidf_ve 100%[===================>] 119.09M  21.2MB/s    in 5.5s    \n\n2021-11-29 06:06:03 (21.5 MB/s) - ‘test_v5701_tfidf_vec.pkl’ saved [124873879/124873879]\n\n--2021-11-29 06:06:04--  https://transfer.sh/zHSiyI/train_v5701_tfidf_vec.pkl\nResolving transfer.sh (transfer.sh)... 144.76.136.153\nConnecting to transfer.sh (transfer.sh)|144.76.136.153|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 499355671 (476M)\nSaving to: ‘train_v5701_tfidf_vec.pkl’\n\ntrain_v5701_tfidf_v 100%[===================>] 476.22M  22.1MB/s    in 21s     \n\n2021-11-29 06:06:25 (22.7 MB/s) - ‘train_v5701_tfidf_vec.pkl’ saved [499355671/499355671]\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2.b. Define Model","metadata":{"id":"PhbVzVLDsMi1"}},{"cell_type":"code","source":"test_data = unpickle_file('./test_v5701_tfidf_vec.pkl')\ntrain_data = unpickle_file('./train_v5701_tfidf_vec.pkl')\n#vocab = unpickle_file('./train_v5701_count_vec.pkl')\n#len(vocab['token_to_idx'])","metadata":{"execution":{"iopub.status.busy":"2021-11-29T06:06:25.932720Z","iopub.execute_input":"2021-11-29T06:06:25.933390Z","iopub.status.idle":"2021-11-29T06:06:27.354914Z","shell.execute_reply.started":"2021-11-29T06:06:25.933310Z","shell.execute_reply":"2021-11-29T06:06:27.353911Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_data[0].shape, test_data[0].shape,","metadata":{"execution":{"iopub.status.busy":"2021-11-29T06:06:38.956742Z","iopub.execute_input":"2021-11-29T06:06:38.957190Z","iopub.status.idle":"2021-11-29T06:06:39.058289Z","shell.execute_reply.started":"2021-11-29T06:06:38.957104Z","shell.execute_reply":"2021-11-29T06:06:39.048413Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(torch.Size([10945, 5701]), torch.Size([2737, 5701]))"},"metadata":{}}]},{"cell_type":"markdown","source":"Describe your model here.","metadata":{"id":"jrUKCNRgsMi1"}},{"cell_type":"code","source":"# Feel free to edit anything in this block\n\nclass MalwareNet(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(MalwareNet, self).__init__()\n        # Layer definitions\n        self.layers = nn.Sequential(\n            nn.Linear(input_dim, 2048),\n            nn.ReLU(),\n            nn.Linear(2048, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n             nn.Linear(512, 256),\n            nn.ReLU(),\n             nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, output_dim)\n        )\n\n    def forward(self, x):\n        # Forward pass\n        x = self.layers(x)\n        return x","metadata":{"id":"ILB2fBRWsMi1","execution":{"iopub.status.busy":"2021-11-29T06:10:36.733238Z","iopub.execute_input":"2021-11-29T06:10:36.733975Z","iopub.status.idle":"2021-11-29T06:10:36.780194Z","shell.execute_reply.started":"2021-11-29T06:10:36.733941Z","shell.execute_reply":"2021-11-29T06:10:36.779221Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## 2.c. Set Hyperparameters","metadata":{"id":"QO1yPx7YsMi1"}},{"cell_type":"code","source":"# Define your hyperparameters here\n\nin_dims = train_data[0][0].shape[0]\nout_dims = len(label_encodings)\n\n# Optimization\nn_epochs = 100\nbatch_size = 512\nlr = 0.001","metadata":{"id":"0k0jLX5ksMi1","execution":{"iopub.status.busy":"2021-11-29T06:10:39.510525Z","iopub.execute_input":"2021-11-29T06:10:39.510818Z","iopub.status.idle":"2021-11-29T06:10:39.562867Z","shell.execute_reply.started":"2021-11-29T06:10:39.510785Z","shell.execute_reply":"2021-11-29T06:10:39.561830Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## 2.d. Train your Model","metadata":{"id":"NtpgVAO1sMi2"}},{"cell_type":"code","source":"# Feel free to edit anything in this block\n\nmodel = MalwareNet(input_dim=in_dims, output_dim=out_dims)\nmodel.train()\n\n# Optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n# Data Loaders\ntrainloader = DataLoader(malware_trainset, batch_size=batch_size, shuffle=True)\ntestloader = DataLoader(malware_testset, batch_size=batch_size, shuffle=False)","metadata":{"id":"Co0W2ifGsMi2","execution":{"iopub.status.busy":"2021-11-29T06:10:39.924051Z","iopub.execute_input":"2021-11-29T06:10:39.924375Z","iopub.status.idle":"2021-11-29T06:10:40.104957Z","shell.execute_reply.started":"2021-11-29T06:10:39.924325Z","shell.execute_reply":"2021-11-29T06:10:40.103756Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"\n# Example:\n# for epoch in range(n_epochs):\n#     ... train ...\n#     ... validate ...\n\ndevice = device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\ncriterion = nn.CrossEntropyLoss() ## since we are doing multiclass classification\nfor epoch in range(n_epochs):\n    y_true = list()\n    y_pred = list()\n    total_loss = 0\n    for batch, targets, lengths in trainloader:\n        \n        ## perform forward pass  \n        batch = batch.type(torch.FloatTensor).to(device)\n        pred = model(batch) \n        preds = torch.max(pred, 1)[1]\n        \n        ## accumulate predictions per batch for the epoch\n        y_pred += list([x.item() for x in preds.detach().cpu().numpy()])\n        targets = torch.LongTensor([x.item() for x in list(targets)])\n        y_true +=  list([x.item() for x in targets.detach().cpu().numpy()])\n        \n        ## compute loss and perform backward pass\n        loss = criterion(pred.to(device), targets.to(device)) ## compute loss \n        optimizer.zero_grad()\n        loss.backward() \n        optimizer.step()\n        \n        ## accumulate train loss\n        total_loss += loss.item() \n        \n    print(f\"[{epoch+1}/{n_epochs}] Train loss: {total_loss} Recall score: {evaluate_preds(y_true, y_pred)}\")\n    \n    ## init placeholder for predictions and groundtruth\n    y_true_val = list()\n    y_pred_val = list()\n    ## perform validation pass\n    for batch, targets, lengths in testloader:\n        ## perform forward pass  \n        batch = batch.type(torch.FloatTensor).to(device)\n        pred = model(batch) \n        preds = torch.max(pred, 1)[1]\n        \n        ## accumulate predictions per batch for the epoch\n        y_pred_val += list([x.item() for x in preds.detach().cpu().numpy()])\n        targets = torch.LongTensor([x.item() for x in list(targets)])\n        y_true_val +=  list([x.item() for x in targets.detach().cpu().numpy()])\n    print(f\"[{epoch+1}/{n_epochs}] Validation Recall score: {evaluate_preds(y_true_val, y_pred_val)}\")\n        ","metadata":{"id":"3afVBWVKsMi2","execution":{"iopub.status.busy":"2021-11-29T06:12:13.200688Z","iopub.execute_input":"2021-11-29T06:12:13.200975Z","iopub.status.idle":"2021-11-29T06:13:26.573513Z","shell.execute_reply.started":"2021-11-29T06:12:13.200944Z","shell.execute_reply":"2021-11-29T06:13:26.572429Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"[1/100] Train loss: 9.962622791528702 Recall score: 0.803799205829268\n[1/100] Validation Recall score: 0.7499912630585566\n[2/100] Train loss: 9.856852293014526 Recall score: 0.8055085897973205\n[2/100] Validation Recall score: 0.75535770420345\n[3/100] Train loss: 9.599773615598679 Recall score: 0.8072449917380176\n[3/100] Validation Recall score: 0.7327680090058621\n[4/100] Train loss: 10.14413496851921 Recall score: 0.7975588153666635\n[4/100] Validation Recall score: 0.7554461827520174\n[5/100] Train loss: 10.26776197552681 Recall score: 0.7999713173298527\n[5/100] Validation Recall score: 0.7577400949817383\n[6/100] Train loss: 10.059604287147522 Recall score: 0.7996030765444558\n[6/100] Validation Recall score: 0.7560749694053804\n[7/100] Train loss: 9.905431270599365 Recall score: 0.803290197375101\n[7/100] Validation Recall score: 0.7549589593231071\n[8/100] Train loss: 9.564646512269974 Recall score: 0.8058022392840503\n[8/100] Validation Recall score: 0.763215730665556\n[9/100] Train loss: 9.612010717391968 Recall score: 0.8095837063333816\n[9/100] Validation Recall score: 0.7547005934647455\n[10/100] Train loss: 9.832004219293594 Recall score: 0.8046237526052391\n[10/100] Validation Recall score: 0.7469532993194499\n[11/100] Train loss: 9.604243218898773 Recall score: 0.8066138808150265\n[11/100] Validation Recall score: 0.7621379179360286\n[12/100] Train loss: 9.652039140462875 Recall score: 0.8083217661954741\n[12/100] Validation Recall score: 0.7572895799333665\n[13/100] Train loss: 9.475706279277802 Recall score: 0.8104230495159941\n[13/100] Validation Recall score: 0.7544761437064522\n[14/100] Train loss: 9.74844229221344 Recall score: 0.8045454173310237\n[14/100] Validation Recall score: 0.7490828184838124\n[15/100] Train loss: 9.451092153787613 Recall score: 0.8095764487237899\n[15/100] Validation Recall score: 0.7688196682827986\n[16/100] Train loss: 9.37676528096199 Recall score: 0.8084145724492526\n[16/100] Validation Recall score: 0.7573949464574081\n[17/100] Train loss: 9.389238744974136 Recall score: 0.8121928981473552\n[17/100] Validation Recall score: 0.7619029919250517\n[18/100] Train loss: 9.297526687383652 Recall score: 0.8138477349244774\n[18/100] Validation Recall score: 0.7598747284713618\n[19/100] Train loss: 9.479447811841965 Recall score: 0.8125632879623585\n[19/100] Validation Recall score: 0.7641847719172883\n[20/100] Train loss: 9.4020337164402 Recall score: 0.8096251166857129\n[20/100] Validation Recall score: 0.7610619501261834\n[21/100] Train loss: 9.596891939640045 Recall score: 0.8057605861986323\n[21/100] Validation Recall score: 0.7537012129130616\n[22/100] Train loss: 9.493472754955292 Recall score: 0.8084811545406877\n[22/100] Validation Recall score: 0.7649494651729174\n[23/100] Train loss: 9.446628004312515 Recall score: 0.810697710303727\n[23/100] Validation Recall score: 0.7552337746688683\n[24/100] Train loss: 9.462036222219467 Recall score: 0.8104780320555383\n[24/100] Validation Recall score: 0.7582761952596966\n[25/100] Train loss: 9.594163805246353 Recall score: 0.8069019464147908\n[25/100] Validation Recall score: 0.7560732032951435\n[26/100] Train loss: 9.382609009742737 Recall score: 0.8123169840223179\n[26/100] Validation Recall score: 0.7551633910882608\n[27/100] Train loss: 10.5141182243824 Recall score: 0.795448063390128\n[27/100] Validation Recall score: 0.7581363647922723\n[28/100] Train loss: 10.477208226919174 Recall score: 0.7912107161266194\n[28/100] Validation Recall score: 0.7538953975695791\n[29/100] Train loss: 16.99111121892929 Recall score: 0.7048353096649164\n[29/100] Validation Recall score: 0.7354454914161466\n[30/100] Train loss: 11.892750322818756 Recall score: 0.7719944439440514\n[30/100] Validation Recall score: 0.7488730227829112\n[31/100] Train loss: 10.324888497591019 Recall score: 0.795622666181038\n[31/100] Validation Recall score: 0.7595885781619568\n[32/100] Train loss: 9.863109558820724 Recall score: 0.8036010575298113\n[32/100] Validation Recall score: 0.7599720693326081\n[33/100] Train loss: 9.608359634876251 Recall score: 0.8047351447995835\n[33/100] Validation Recall score: 0.7655576427091453\n[34/100] Train loss: 9.49992847442627 Recall score: 0.8082478057002532\n[34/100] Validation Recall score: 0.7522979405451498\n[35/100] Train loss: 9.38829717040062 Recall score: 0.8073628419387893\n[35/100] Validation Recall score: 0.761039397159949\n[36/100] Train loss: 9.273954659700394 Recall score: 0.8107666024082831\n[36/100] Validation Recall score: 0.7518918125226532\n[37/100] Train loss: 9.51368311047554 Recall score: 0.80421993741765\n[37/100] Validation Recall score: 0.762225451728151\n[38/100] Train loss: 9.45599702000618 Recall score: 0.810380044332714\n[38/100] Validation Recall score: 0.7612623902851838\n[39/100] Train loss: 9.240584045648575 Recall score: 0.8139937001001447\n[39/100] Validation Recall score: 0.7601043068951134\n[40/100] Train loss: 9.199834078550339 Recall score: 0.8126149586641382\n[40/100] Validation Recall score: 0.7562714142472431\n[41/100] Train loss: 9.225174933671951 Recall score: 0.8118853980829789\n[41/100] Validation Recall score: 0.7648763498948961\n[42/100] Train loss: 9.268993645906448 Recall score: 0.8136081035258265\n[42/100] Validation Recall score: 0.7602340952217166\n[43/100] Train loss: 9.356137186288834 Recall score: 0.8109843390537262\n[43/100] Validation Recall score: 0.7598295357811328\n[44/100] Train loss: 9.182995587587357 Recall score: 0.8138064699040204\n[44/100] Validation Recall score: 0.7575185289426878\n[45/100] Train loss: 9.236146956682205 Recall score: 0.8127286853193858\n[45/100] Validation Recall score: 0.7605474615900558\n[46/100] Train loss: 9.173011511564255 Recall score: 0.8135056701656025\n[46/100] Validation Recall score: 0.7613639677724471\n[47/100] Train loss: 9.371939897537231 Recall score: 0.8110978817871988\n[47/100] Validation Recall score: 0.7647107924606368\n[48/100] Train loss: 9.49497303366661 Recall score: 0.8080434527522815\n[48/100] Validation Recall score: 0.7591863924879485\n[49/100] Train loss: 9.250415176153183 Recall score: 0.811688384222904\n[49/100] Validation Recall score: 0.7625462981659253\n[50/100] Train loss: 9.138443529605865 Recall score: 0.8155906718971082\n[50/100] Validation Recall score: 0.7561922788341148\n[51/100] Train loss: 9.337086230516434 Recall score: 0.8117495523332158\n[51/100] Validation Recall score: 0.7602233023152509\n[52/100] Train loss: 9.078617930412292 Recall score: 0.8159644785410466\n[52/100] Validation Recall score: 0.7699368694069408\n[53/100] Train loss: 9.045452862977982 Recall score: 0.8180607589605909\n[53/100] Validation Recall score: 0.756080634941279\n[54/100] Train loss: 9.130762815475464 Recall score: 0.8140040262478152\n[54/100] Validation Recall score: 0.7605171485709797\n[55/100] Train loss: 9.1163010597229 Recall score: 0.81456851217328\n[55/100] Validation Recall score: 0.7609165276272848\n[56/100] Train loss: 9.175871312618256 Recall score: 0.814213896533116\n[56/100] Validation Recall score: 0.7577569265664953\n[57/100] Train loss: 9.228421181440353 Recall score: 0.8127634040284025\n[57/100] Validation Recall score: 0.7617571021763637\n[58/100] Train loss: 9.191090017557144 Recall score: 0.814622223717298\n[58/100] Validation Recall score: 0.7590215128452125\n[59/100] Train loss: 9.050934463739395 Recall score: 0.8179178953885582\n[59/100] Validation Recall score: 0.7688260248126125\n[60/100] Train loss: 9.006253719329834 Recall score: 0.8159241829706019\n[60/100] Validation Recall score: 0.7667034289106927\n[61/100] Train loss: 8.956939309835434 Recall score: 0.8186592902250462\n[61/100] Validation Recall score: 0.7602585896286571\n[62/100] Train loss: 8.993493378162384 Recall score: 0.8175962382251075\n[62/100] Validation Recall score: 0.7687249332355639\n[63/100] Train loss: 9.026254177093506 Recall score: 0.8171356234142024\n[63/100] Validation Recall score: 0.7614024232699577\n[64/100] Train loss: 8.946435660123825 Recall score: 0.8181719101853762\n[64/100] Validation Recall score: 0.7635031959272561\n[65/100] Train loss: 8.86920565366745 Recall score: 0.8171964336694844\n[65/100] Validation Recall score: 0.7629927555063236\n[66/100] Train loss: 8.87405526638031 Recall score: 0.8180006531607609\n[66/100] Validation Recall score: 0.7580883872779143\n[67/100] Train loss: 9.039924025535583 Recall score: 0.8177419437442468\n[67/100] Validation Recall score: 0.7677613407791164\n[68/100] Train loss: 8.830060929059982 Recall score: 0.8212708349546605\n[68/100] Validation Recall score: 0.7628339227881555\n[69/100] Train loss: 9.010544031858444 Recall score: 0.8184096545261701\n[69/100] Validation Recall score: 0.7622539356602465\n[70/100] Train loss: 9.062630712985992 Recall score: 0.8164187008016015\n[70/100] Validation Recall score: 0.7570073632644636\n[71/100] Train loss: 8.91518098115921 Recall score: 0.8185541748925939\n[71/100] Validation Recall score: 0.7623165816998212\n[72/100] Train loss: 8.806161880493164 Recall score: 0.8190189766064812\n[72/100] Validation Recall score: 0.7557950104251503\n[73/100] Train loss: 9.10058417916298 Recall score: 0.8149265982961296\n[73/100] Validation Recall score: 0.7618319946712746\n[74/100] Train loss: 8.962539941072464 Recall score: 0.8195495712298815\n[74/100] Validation Recall score: 0.7556157627373733\n[75/100] Train loss: 9.362182855606079 Recall score: 0.8133606451256801\n[75/100] Validation Recall score: 0.766872337592144\n[76/100] Train loss: 8.84645301103592 Recall score: 0.8194076983404253\n[76/100] Validation Recall score: 0.7644625914186907\n[77/100] Train loss: 8.868198156356812 Recall score: 0.8203073495739476\n[77/100] Validation Recall score: 0.7503712916675579\n[78/100] Train loss: 9.049733728170395 Recall score: 0.8173335008273183\n[78/100] Validation Recall score: 0.7514556045155643\n[79/100] Train loss: 8.889204621315002 Recall score: 0.8167952024490057\n[79/100] Validation Recall score: 0.7629174791569309\n[80/100] Train loss: 8.768473595380783 Recall score: 0.8204430424405278\n[80/100] Validation Recall score: 0.7653228428783949\n[81/100] Train loss: 8.798381268978119 Recall score: 0.8211670254710727\n[81/100] Validation Recall score: 0.7598852831069921\n[82/100] Train loss: 8.808314770460129 Recall score: 0.8180432608278085\n[82/100] Validation Recall score: 0.7570453052612216\n[83/100] Train loss: 8.964827001094818 Recall score: 0.8186658033386675\n[83/100] Validation Recall score: 0.7707581267900118\n[84/100] Train loss: 8.716299891471863 Recall score: 0.817978074396704\n[84/100] Validation Recall score: 0.7654767974124221\n[85/100] Train loss: 8.651638269424438 Recall score: 0.8204461785818783\n[85/100] Validation Recall score: 0.7661802954351458\n[86/100] Train loss: 8.638897150754929 Recall score: 0.8201141995040274\n[86/100] Validation Recall score: 0.7638711338011273\n[87/100] Train loss: 8.638389438390732 Recall score: 0.8222809537142597\n[87/100] Validation Recall score: 0.7622246706209588\n[88/100] Train loss: 8.653479635715485 Recall score: 0.8213335249651361\n[88/100] Validation Recall score: 0.768605371643852\n[89/100] Train loss: 8.819876074790955 Recall score: 0.8198687707568423\n[89/100] Validation Recall score: 0.7667083050880455\n[90/100] Train loss: 8.942859143018723 Recall score: 0.8171614109592168\n[90/100] Validation Recall score: 0.7607632550271758\n[91/100] Train loss: 8.80745193362236 Recall score: 0.8205173355320469\n[91/100] Validation Recall score: 0.7591774454531665\n[92/100] Train loss: 8.750962764024734 Recall score: 0.8224383959532231\n[92/100] Validation Recall score: 0.7655381393124309\n[93/100] Train loss: 8.719333171844482 Recall score: 0.820122126276653\n[93/100] Validation Recall score: 0.7686056235463955\n[94/100] Train loss: 8.774372726678848 Recall score: 0.8225969937987271\n[94/100] Validation Recall score: 0.7623923913378979\n[95/100] Train loss: 8.629417777061462 Recall score: 0.8227612048893242\n[95/100] Validation Recall score: 0.7635532771905693\n[96/100] Train loss: 8.605426400899887 Recall score: 0.8243479891260497\n[96/100] Validation Recall score: 0.7629467723282692\n[97/100] Train loss: 8.755124986171722 Recall score: 0.822449532291647\n[97/100] Validation Recall score: 0.7674079978278053\n[98/100] Train loss: 8.710477977991104 Recall score: 0.8217187819113437\n[98/100] Validation Recall score: 0.7633263092733097\n[99/100] Train loss: 8.649472564458847 Recall score: 0.8218369044608801\n[99/100] Validation Recall score: 0.7692765483814454\n[100/100] Train loss: 8.546666324138641 Recall score: 0.823693103896128\n[100/100] Validation Recall score: 0.7606737527273986\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2.e. Evaluate model","metadata":{"id":"Z5w-7O5jsMi2"}},{"cell_type":"code","source":"#\n#\n# ------- Your Code -------\n#\n# ","metadata":{"id":"CMbzX1h4sMi2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.f. Save Model + Data","metadata":{"id":"U_GE1BQjsMi3"}},{"cell_type":"code","source":"#\n#\n# ------- Your Code -------\n#\n# ","metadata":{"id":"4Xvo--GIsMi3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{"id":"W-UQbfCrsMi3"}},{"cell_type":"markdown","source":"# 3. Analysis","metadata":{"id":"uLD6dur1sMi3"}},{"cell_type":"markdown","source":"## 3.a. Summary: Main Results\n\nSummarize your approach and results here","metadata":{"id":"1QRKxLsPsMi3"}},{"cell_type":"markdown","source":"## 3.b. Discussion\n\nEnter your final summary here.\n\nFor instance, you can address:\n- What was the performance you obtained with the simplest approach?\n- Which vectorized input representations helped more than the others?\n- Which malwares are difficult to detect and why?\n- Which approach do you recommend to perform malware classification?","metadata":{"id":"dNKqJAoasMi4"}},{"cell_type":"code","source":"","metadata":{"id":"xYGR9ID1sMi4"},"execution_count":null,"outputs":[]}]}