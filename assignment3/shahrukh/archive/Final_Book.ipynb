{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Repos\\ml-cybersecurity\\assignment3\\shahrukh\\data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file data already exists.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open 'train.tar.gz'\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Repos\\ml-cybersecurity\\assignment3\\shahrukh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tar: Error opening archive: Failed to open 'test.tar.gz'\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "%cd data\n",
    "!wget https://nextcloud.mpi-klsb.mpg.de/index.php/s/pJrRGzm2So2PMZm/download -O train.tar.gz \n",
    "!tar xzf train.tar.gz\n",
    "!wget https://nextcloud.mpi-klsb.mpg.de/index.php/s/zN3yeWzQB3i5WqE/download -O test.tar.gz \n",
    "!tar xzf test.tar.gz\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKo3S_fOsMim"
   },
   "source": [
    "# ML in Cybersecurity: Task 3\n",
    "\n",
    "## Team\n",
    "  * **Team name**:  *R2D2C3P0BB8*\n",
    "  * **Members**:  <br/> **Navdeeppal Singh (s8nlsing@stud.uni-saarland.de)** <br/> **Shahrukh Khan (shkh00001@stud.uni-saarland.de)** <br/> **Mahnoor Shahid (mash00001@stud.uni-saarland.de)**\n",
    "\n",
    "\n",
    "## Logistics\n",
    "  * **Due date**: 9th December 2021, 23:59:59\n",
    "  * Email the completed notebook to: `mlcysec_ws2022_staff@lists.cispa.saarland`\n",
    "  * Complete this in **teams of 3**\n",
    "  * Feel free to use the forum to discuss.\n",
    "  \n",
    "## Timeline\n",
    "  * 26-Nov-2021: hand-out\n",
    "  * **09-Dec-2021**: Email completed notebook\n",
    "  \n",
    "  \n",
    "## About this Project\n",
    "In this project, you will explore an application of ML to a popular task in cybersecurity: malware classification.\n",
    "You will be presented with precomputed behaviour analysis reports of thousands of program binaries, many of which are malwares.\n",
    "Your goal is to train a malware detector using this behavioural reports.\n",
    "\n",
    "\n",
    "## A Note on Grading\n",
    "The grading for this project will depend on:\n",
    " 1. Vectorizing Inputs\n",
    "   * Obtaining a reasonable vectorized representations of the input data (a file containing a sequence of system calls)\n",
    "   * Understanding the influence these representations have on your model\n",
    " 1. Classification Model  \n",
    "   * Following a clear ML pipeline\n",
    "   * Obtaining reasonable performances (>60\\%) on held-out test set\n",
    "   * Choice of evaluation metric\n",
    "   * Visualizing loss/accuracy curves\n",
    " 1. Analysis\n",
    "   * Which methods (input representations/ML models) work better than the rest and why?\n",
    "   * Which hyper-parameters and design-choices were important in each of your methods?\n",
    "   * Quantifying influence of these hyper-parameters on loss and/or validation accuracies\n",
    "   * Trade-offs between methods, hyper-parameters, design-choices\n",
    "   * Anything else you find interesting (this part is open-ended)\n",
    "\n",
    "\n",
    "## Grading Details\n",
    " * 40 points: Vectorizing input data (each input = behaviour analysis file in our case)\n",
    " * 40 points: Training a classification model\n",
    " * 15 points: Analysis/Discussion\n",
    " * 5 points: Clean code\n",
    " \n",
    "## Filling-in the Notebook\n",
    "You'll be submitting this very notebook that is filled-in with your code and analysis. Make sure you submit one that has been previously executed in-order. (So that results/graphs are already visible upon opening it). \n",
    "\n",
    "The notebook you submit **should compile** (or should be self-contained and sufficiently commented). Check tutorial 1 on how to set up the Python3 environment.\n",
    "\n",
    "\n",
    "**The notebook is your project report. So, to make the report readable, omit code for techniques/models/things that did not work. You can use the final summary to provide a report about these.**\n",
    "\n",
    "It is extremely important that you **do not** re-order the existing sections. Apart from that, the code blocks that you need to fill-in are given by:\n",
    "```\n",
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "```\n",
    "Feel free to break this into multiple-cells. It's even better if you interleave explanations and code-blocks so that the entire notebook forms a readable \"story\".\n",
    "\n",
    "\n",
    "## Code of Honor\n",
    "We encourage discussing ideas and concepts with other students to help you learn and better understand the course content. However, the work you submit and present **must be original** and demonstrate your effort in solving the presented problems. **We will not tolerate** blatantly using existing solutions (such as from the internet), improper collaboration (e.g., sharing code or experimental data between groups) and plagiarism. If the honor code is not met, no points will be awarded.\n",
    "\n",
    " \n",
    " ## Versions\n",
    "  * v1.1: Updated deadline\n",
    "  * v1.0: Initial notebook\n",
    "  \n",
    "  ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uyCiLbXbsMiq"
   },
   "outputs": [],
   "source": [
    "import time \n",
    " \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import json \n",
    "import time \n",
    "import pickle \n",
    "import sys \n",
    "import csv \n",
    "import os \n",
    "import os.path as osp \n",
    "import shutil \n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import display, HTML\n",
    " \n",
    "%matplotlib inline \n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots \n",
    "plt.rcParams['image.interpolation'] = 'nearest' \n",
    "plt.rcParams['image.cmap'] = 'gray' \n",
    " \n",
    "# for auto-reloading external modules \n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7dHogdxRsMis"
   },
   "outputs": [],
   "source": [
    "# Some suggestions of our libraries that might be helpful for this project\n",
    "from collections import Counter          # an even easier way to count\n",
    "from multiprocessing import Pool         # for multiprocessing\n",
    "from tqdm import tqdm                    # fancy progress bars\n",
    "\n",
    "# Load other libraries here.\n",
    "from sklearn.metrics import recall_score\n",
    "# Keep it minimal! We should be easily able to reproduce your code.\n",
    "\n",
    "# We preload pytorch as an example\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJWkh3GUsMit"
   },
   "source": [
    "# Setup\n",
    "\n",
    "  * Download the datasets: [train](https://nextcloud.mpi-klsb.mpg.de/index.php/s/pJrRGzm2So2PMZm) (128M) and [test](https://nextcloud.mpi-klsb.mpg.de/index.php/s/zN3yeWzQB3i5WqE) (92M)\n",
    "  * Unpack them under `./data/train` and `./data/test`\n",
    "  * Hint: you can execute shell scripts from notebooks using the `!` prefix, e.g., `! wget <url>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wlhs4w44sMit"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train examples (Should be 13682) :  13682\n",
      "# test examples (Should be 13682) :  10000\n"
     ]
    }
   ],
   "source": [
    "# Check that you are prepared with the data\n",
    "try:\n",
    "    print(f\"# train examples (Should be 13682) : \", len(os.listdir('./data/train')))\n",
    "    print(f\"# test examples (Should be 13682) : \", len(os.listdir('./data/test')))\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"You don't have the data!\")\n",
    "# ! printf '# train examples (Should be 13682) : '; ls data/train | wc -l\n",
    "# ! printf '# test  examples (Should be 10000) : '; ls data/test | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zh1-JHNIsMiv"
   },
   "source": [
    "Now that you're set, let's briefly look at the data you have been handed.\n",
    "Each file encodes the behavior report of a program (potentially a malware), using an encoding scheme called \"The Malware Instruction Set\" (MIST for short).\n",
    "At this point, we highly recommend you briefly read-up Sec. 2 of the [MIST](http://www.mlsec.org/malheur/docs/mist-tr.pdf) documentation.\n",
    "\n",
    "You will find each file named as `filename.<malwarename>`:\n",
    "```\n",
    "Â» ls data/train | head\n",
    "00005ecc06ae3e489042e979717bb1455f17ac9d.NothingFound\n",
    "0008e3d188483aeae0de62d8d3a1479bd63ed8c9.Basun\n",
    "000d2eea77ee037b7ef99586eb2f1433991baca9.Patched\n",
    "000d996fa8f3c83c1c5568687bb3883a543ec874.Basun\n",
    "0010f78d3ffee61101068a0722e09a98959a5f2c.Basun\n",
    "0013cd0a8febd88bfc4333e20486bd1a9816fcbf.Basun\n",
    "0014aca72eb88a7f20fce5a4e000c1f7fff4958a.Texel\n",
    "001ffc75f24a0ae63a7033a01b8152ba371f6154.Texel\n",
    "0022d6ba67d556b931e3ab26abcd7490393703c4.Basun\n",
    "0028c307a125cf0fdc97d7a1ffce118c6e560a70.Swizzor\n",
    "...\n",
    "```\n",
    "and within each file, you will see a sequence of individual systems calls monitored duing the run-time of the binary - a malware named 'Basun' in the case:\n",
    "```\n",
    "Â» head data/train/000d996fa8f3c83c1c5568687bb3883a543ec874.Basun\n",
    "# process 000006c8 0000066a 022c82f4 00000000 thread 0001 #\n",
    "02 01 | 000006c8 0000066a 00015000\n",
    "02 02 | 00006b2c 047c8042 000b9000\n",
    "02 02 | 00006b2c 047c8042 00108000\n",
    "02 02 | 00006b2c 047c8042 00153000\n",
    "02 02 | 00006b2c 047c8042 00091000\n",
    "02 02 | 00006b2c 047c8042 00049000\n",
    "02 02 | 00006b2c 047c8042 000aa000\n",
    "02 02 | 00006b2c 047c8042 00092000\n",
    "02 02 | 00006b2c 047c8042 00011000\n",
    "...\n",
    "```\n",
    "(**Note**: Please ignore the first line that begins with `# process ...`.)\n",
    "\n",
    "Your task in this project is to train a malware detector, which given the sequence of system calls (in the MIST-formatted file like above), predicts one of 10 classes: `{ Agent, Allaple, AutoIt, Basun, NothingFound, Patched, Swizzor, Texel, VB, Virut }`, where `NothingFound` roughly represents no malware is present.\n",
    "In terms of machine learning terminology, your malware detector $F: X \\rightarrow Y$ should learn a mapping from the MIST-encoded behaviour report (the input $x \\in X$) to the malware class $y \\in Y$.\n",
    "\n",
    "Consequently, you will primarily tackle two challenges in this project:\n",
    "  1. \"Vectorizing\" the input data i.e., representing each input (file) as a tensor\n",
    "  1. Training an ML model\n",
    "  \n",
    "\n",
    "### Some tips:\n",
    "  * Begin with an extremely simple representation/ML model and get above chance-level classification performance\n",
    "  * Choose your evaluation metric wisely\n",
    "  * Save intermediate computations (e.g., a token to index mapping). This will avoid you parsing the entire dataset for every experiment\n",
    "  * Try using `multiprocessing.Pool` to parallelize your `for` loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8xloR8dsMiv"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXnyDAdbsMiw"
   },
   "source": [
    "# 1. Vectorize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ia-xFIQtsMiw"
   },
   "source": [
    "## 1.a. Load Raw Data\n",
    "## => We converted list of list lines to string to save memory in order to load entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZScY-C2IsMiw"
   },
   "outputs": [],
   "source": [
    "def load_content(filepath):\n",
    "    '''Given a filepath, returns (content, classname), where content = [list of lines in file]'''\n",
    "    ## load file content\n",
    "    file = open(filepath, \"r\")\n",
    "    file_lines = file.read()\n",
    "    ## here converted list of list lines to string to save memory in order to load entire dataset\n",
    "    lines = \"\\n\".join(file_lines.splitlines())\n",
    "    file.close()\n",
    "\n",
    "    ## extracting label\n",
    "    label = filepath.split(\".\")[-1]\n",
    "    return lines, label\n",
    "\n",
    "\n",
    "def load_data(data_path, nworkers=10):\n",
    "    '''Returns each data sample as a tuple (x, y), x = sequence of strings (i.e., syscalls), y = malware program class'''\n",
    "    raw_data_samples = []\n",
    "    \n",
    "    file_paths = [f\"{data_path}/{filename}\" for filename in os.listdir(data_path)]\n",
    "    pool = Pool(processes=nworkers)\n",
    " \n",
    "    raw_data_samples = pool.map(load_content, file_paths)\n",
    "    return raw_data_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7BSUdq_-sMiw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_path = './data/train'\n",
    "test_path = './data/test'\n",
    "n_workers = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GG6tcSNJsMiw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading training data ... \n"
     ]
    }
   ],
   "source": [
    "project_mode = 'trainval'    # trainval, traintest, debug, eval\n",
    "np.random.seed(123)          # To perform the same split across multiple runs\n",
    "\n",
    "## in trainval mode we use test_raw_samples variable to hold validation dataset\n",
    "train_raw_samples, test_raw_samples = [], []\n",
    " \n",
    "if project_mode == 'trainval':\n",
    "    print('=> Loading training data ... ')\n",
    "    train_raw_samples = load_data(Path(train_path), nworkers=n_workers)\n",
    "    # Split data into train and validation set\n",
    "    np.random.shuffle(train_raw_samples)\n",
    "    train_raw_samples, test_raw_samples = train_raw_samples[:int(len(train_raw_samples)*0.8)], train_raw_samples[int(len(train_raw_samples)*0.8):]\n",
    "\n",
    "elif project_mode == 'traintest':\n",
    "    ## loading train and test set\n",
    "    print('=> Loading training data ... ')\n",
    "    train_raw_samples = load_data(Path(train_path), nworkers=n_workers)\n",
    "    print('=> Loading testing data ... ')\n",
    "    test_raw_samples = load_data(Path(test_path), nworkers=n_workers)\n",
    "    \n",
    "elif project_mode == 'debug':\n",
    "    print('=> Loading training data ... ')\n",
    "    train_raw_samples = load_data(Path(train_path), nworkers=n_workers)\n",
    "    print('=> Loading testing data ... ')\n",
    "    test_raw_samples = load_data(Path(test_path), nworkers=n_workers)\n",
    "    # Optional, use a small subset of the training and validation data for fast debugging\n",
    "    train_set, test_set = train_raw_samples[:100], test_raw_samples[:100]\n",
    "\n",
    "elif project_mode == 'eval':\n",
    "    ## load only test set for evaluating the model\n",
    "    print('=> Loading testing data ... ')\n",
    "    test_raw_samples = load_data(Path(test_path), nworkers=n_workers)\n",
    "\n",
    "else:\n",
    "    raise ValueError('Unrecognized mode')\n",
    "    \n",
    "print('=> # Train samples = ', len(train_raw_samples))\n",
    "print('=> # Test  samples = ', len(test_raw_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjiNiOVHsMiy"
   },
   "source": [
    "## 1.b. Vectorize: Setup\n",
    "\n",
    "Make one pass over the inputs to identify relevant features/tokens.\n",
    "\n",
    "Suggestion:\n",
    "  - identify tokens (e.g., unigrams, bigrams)\n",
    "  - create a token -> index (int) mapping. Note that you might have a >10K unique tokens. So, you will have to choose a suitable \"vocabulary\" size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DAtmzL6PsMiy"
   },
   "outputs": [],
   "source": [
    "# Feel free to edit anything in this block\n",
    "\n",
    "def get_key_idx_map(counter, vocab_size, ukn_token='_ukn_'):\n",
    "    \"\"\"counter is a mapping: token -> count\n",
    "    build vectorizer using vocab_size most common elements\"\"\"\n",
    "    key_to_idx, idx_to_key = dict(), dict()\n",
    "    \n",
    "    for idx, (key, value) in tqdm(enumerate(list(train_counter.items())[:vocab_size-1])):\n",
    "        ## perform mapping for token\n",
    "        key_to_idx[key] = idx\n",
    "        idx_to_key[idx] = key\n",
    "    ## perform mapping for unk token at the end\n",
    "    key_to_idx[ukn_token] = vocab_size - 1\n",
    "    idx_to_key[vocab_size - 1] = ukn_token\n",
    "    \n",
    "    return key_to_idx, idx_to_key\n",
    "\n",
    "def preprocess(data):\n",
    "    \"\"\"concatenating all sys calls to single string for tokenization\n",
    "    removing extraneous information such as lines with '# process', white spaces and '|' characters\"\"\"\n",
    "    for i, (X,y) in enumerate(tqdm(data)):\n",
    "        example = \"\"\n",
    "        for line in X.split(\"\\n\"):\n",
    "            ## skip lines containing '# process'\n",
    "            if \"# process\" in line:\n",
    "                continue\n",
    "            ## remove extraneous white spaces and \n",
    "            example += line.replace(\"|\",\"\").replace(\"  \", \" \").strip() + \" \"\n",
    "        example = example.strip()\n",
    "        ## assign preprocessed sample\n",
    "        data[i] = (example, y)\n",
    "    return data\n",
    "        \n",
    "def count_words(data):\n",
    "    \"\"\"\n",
    "    count token occurences for building vocabulary later\n",
    "    \"\"\"\n",
    "    counter = {}\n",
    "    for X,y in tqdm(data):\n",
    "        counts = dict(Counter(X.split()))\n",
    "        counter = dict(counter, **counts)\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'project_mode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3960/4222230828.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \"\"\"\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mproject_mode\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"eval\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mtrain_raw_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_raw_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtrain_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_raw_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'project_mode' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Preprocessing both train and test set and\n",
    "Creating token counter for building vocabulary on train set\n",
    "\"\"\"\n",
    "train_counter = None\n",
    "if project_mode != \"eval\":\n",
    "    train_raw_samples = preprocess(train_raw_samples)\n",
    "    train_counter = count_words(train_raw_samples)\n",
    "test_raw_samples = preprocess(test_raw_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code for finding appropriate threshold for setting `MAX_VOCAB_SIZE`\n",
    "def choose_vocab_size(min_frequency_threshold=10):\n",
    "    count = 0\n",
    "    for value,key in sorted([(value,key) for (key,value) in train_counter.items()], reverse=True):\n",
    "        if value > min_frequency_threshold:\n",
    "            count+=1\n",
    "    print(f\"Number of tokens are {count} for min. frequency threshold={min_frequency_threshold}\")\n",
    "#choose_vocab_size(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3960/2301515796.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## sorting the counters wrt to count values in decending order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m  \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "## sorting the counters wrt to count values in decending order\n",
    "train_counter = {key:value for value, key in  sorted([(value,key) for (key,value) in train_counter.items()], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSMEAcgvsMiy"
   },
   "outputs": [],
   "source": [
    "# Feel free to edit anything in this block\n",
    "## By keeping a minimum count threshold of 10 we get 6316 most frequent tokens in train dataset\n",
    "## adding one to MAX_VOCAB_SIZE for _ukn_ token\n",
    "MAX_VOCAB_SIZE = 5701\n",
    "\n",
    "token_to_idx, idx_to_token = get_key_idx_map(train_counter, MAX_VOCAB_SIZE)\n",
    "\n",
    "# Save vocab to file\n",
    "out_path = 'application_vocab_{}.pkl'.format(MAX_VOCAB_SIZE)\n",
    "with open(out_path, 'wb') as wf:\n",
    "    dct = {'token_to_idx': token_to_idx,\n",
    "          'idx_to_token': idx_to_token}\n",
    "    pickle.dump(dct, wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUefzVjFsMiy"
   },
   "source": [
    "## 1.c. Vectorize Data\n",
    "\n",
    "Use the (token $\\rightarrow$ index) mapping you created before to vectorize your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffJJ0XOvsMiz"
   },
   "outputs": [],
   "source": [
    "def sample_to_idx(sample):\n",
    "    idx_sample = []\n",
    "    for token in sample.split(' '):\n",
    "        if token not in token_to_idx:\n",
    "                token = '_ukn_'\n",
    "        idx_sample.append(token_to_idx[token])\n",
    "    return idx_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define mapping for labels\n",
    "label_encodings = {'Virut': 0,\n",
    " 'Swizzor': 1,\n",
    " 'Agent': 2,\n",
    " 'Patched': 3,\n",
    " 'Allaple': 4,\n",
    " 'Texel': 5,\n",
    " 'Basun': 6,\n",
    " 'AutoIt': 7,\n",
    " 'NothingFound': 8,\n",
    " 'VB': 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CD5IqJmXsMiz"
   },
   "outputs": [],
   "source": [
    "def vectorize_raw_samples_bow(raw_samples, vocab_length, nworkers=10):\n",
    "    '''\n",
    "    Sharhrukh will add this part\n",
    "    '''\n",
    "    vectorized_samples = []\n",
    "    labels = []\n",
    "    lengths = []\n",
    "    for idx, (X,y) in tqdm(enumerate(raw_samples)):\n",
    "        vectorized_sample = []\n",
    "        ## map labeks to ids\n",
    "        label = label_encodings[y]\n",
    "        ## map tokens to ids\n",
    "        X_idx = sample_to_idx(X)\n",
    "        \n",
    "        ## initializing placeholder vector with unknown tokens equivalent to max_length\n",
    "        vector_sample = [0] * vocab_length\n",
    "        \n",
    "        ## creating Bag of Words Vectors \n",
    "        for index, val in enumerate(set(X_idx)):\n",
    "            vector_sample[val] = 1\n",
    "        sequence_length = len(X_idx)\n",
    "        \n",
    "        ## append sample to respective lists\n",
    "        vectorized_samples.append(vector_sample)\n",
    "        labels.append(label)\n",
    "        lengths.append(sequence_length)\n",
    "    \n",
    "    return (torch.LongTensor(vectorized_samples), torch.LongTensor(labels), torch.LongTensor(lengths))\n",
    "\n",
    "\n",
    "\n",
    "def vectorize_raw_samples_count_vectorizer(raw_samples, vocab_length, nworkers=10):\n",
    "    '''\n",
    "    Sharhrukh will add this part\n",
    "    '''\n",
    "    vectorized_samples = []\n",
    "    labels = []\n",
    "    lengths = []\n",
    "    for idx, (X,y) in tqdm(enumerate(raw_samples)):\n",
    "        vectorized_sample = []\n",
    "        ## map labeks to ids\n",
    "        label = label_encodings[y]\n",
    "        ## map tokens to ids\n",
    "        X_idx = sample_to_idx(X)\n",
    "        \n",
    "        ## initializing placeholder vector with unknown tokens equivalent to max_length\n",
    "        vector_sample = [0] * vocab_length\n",
    "        ## compute counts\n",
    "        counts = dict(Counter(X_idx))\n",
    "        ## creating Count Vectors \n",
    "        for index, (key, val) in enumerate(counts.items()):\n",
    "            vector_sample[key] = val\n",
    "        sequence_length = len(X_idx)\n",
    "        \n",
    "        ## append sample to respective lists\n",
    "        vectorized_samples.append(vector_sample)\n",
    "        labels.append(label)\n",
    "        lengths.append(sequence_length)\n",
    "    \n",
    "    return (torch.LongTensor(vectorized_samples), torch.LongTensor(labels), torch.LongTensor(lengths))\n",
    "\n",
    "\n",
    "\n",
    "def vectorize_raw_samples_tfidf(raw_samples, vocab_length, nworkers=10):\n",
    "    '''\n",
    "    Sharhrukh will add this part\n",
    "    '''\n",
    "    labels = []\n",
    "    lengths = []\n",
    "    tf_samples = [] ## term frequency vector for each sample\n",
    "    for idx, (X,y) in tqdm(enumerate(raw_samples)):\n",
    "        vectorized_sample = []\n",
    "        ## map labeks to ids\n",
    "        label = label_encodings[y]\n",
    "        ## map tokens to ids\n",
    "        X_idx = sample_to_idx(X)\n",
    "        \n",
    "        ## initializing placeholder vector with unknown tokens equivalent to max_length\n",
    "        vector_sample = [0] * vocab_length\n",
    "        ## compute counts\n",
    "        counts = dict(Counter(X_idx))\n",
    "        ## creating Count Vectors \n",
    "        for index, (key, val) in enumerate(counts.items()):\n",
    "            vector_sample[key] = val\n",
    "        sequence_length = len(X_idx)\n",
    "        ## compute term frequencies 'tf => # of times term in the doc / total words in the doc'\n",
    "        term_frequencies = np.array(vector_sample) / sequence_length\n",
    "        ## append sample to respective lists\n",
    "        labels.append(label)\n",
    "        lengths.append(sequence_length)\n",
    "        tf_samples.append(term_frequencies)\n",
    "    \n",
    "    # compute idf\n",
    "    # 1. computing BOW matrix \n",
    "    bow = np.zeros(shape=(len(raw_samples), vocab_length))\n",
    "    for i in range(len(tf_samples)):\n",
    "        for j in range(vocab_length):\n",
    "            if tf_samples[i][j] > 0:\n",
    "                bow[i,j] = 1\n",
    "    # 2, compute idf scores 'idf(t) => log( ((1 + # of docs)/ # of docs with term t + 1) + 1 ) '\n",
    "    idf = [np.log(((1+len(raw_samples))/(1+sum(bow[:, i])))+1) for i in range(vocab_length)]\n",
    "    \n",
    "    # compute tf-idf => tf * idf\n",
    "    tf = np.array(tf_samples)\n",
    "    tf_idf = np.zeros(shape=(len(raw_samples), vocab_length))\n",
    "    for i in range(vocab_length):\n",
    "        tf_idf[:, i] = tf[:, i] * idf[i]\n",
    "    return (torch.DoubleTensor(tf_idf), torch.LongTensor(labels), torch.LongTensor(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BtS10ASbsMi0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('=> Processing: Train')\n",
    "train_data = vectorize_raw_samples_bow(train_raw_samples, vocab_length=MAX_VOCAB_SIZE)\n",
    "print()\n",
    "print('=> Processing: Test')\n",
    "test_data = vectorize_raw_samples_bow(test_raw_samples, vocab_length=MAX_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = unpickle_file('./test_v5701_count_vec.pkl')\n",
    "train_data = unpickle_file('./train_v5701_count_vec.pkl')\n",
    "#vocab = unpickle_file('./application_vocab_5701.pkl')\n",
    "#len(vocab['token_to_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "0XOHr24usMi0"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Suggestions: \n",
    "#\n",
    "# (a) You can use torch.utils.data.TensorDataset to represent the tensors you created previously\n",
    "# trainset = TensorDataset(train_x, train_y)\n",
    "# testset = TensorDataset(test_x, test_y)\n",
    "#\n",
    "# (b) Store your datasets to disk so that you do not need to precompute it every time\n",
    "\n",
    "\"\"\"\n",
    "Standard Pytorch Dataset class for loading datasets.\n",
    "\"\"\"\n",
    "class MalwareDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_tensor, target_tensor, length_tensor):\n",
    "        \"\"\"\n",
    "        initializes  and populates the the length, data and target tensors, and raw texts list\n",
    "        \"\"\"\n",
    "        assert data_tensor.size(0) == target_tensor.size(0) == length_tensor.size(0)\n",
    "        self.data_tensor = data_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "        self.length_tensor = length_tensor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        returns the tuple of data tensor, targets, lengths of sequences tensor\n",
    "        \"\"\"\n",
    "        return self.data_tensor[index], self.target_tensor[index], self.length_tensor[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        returns the length of the data tensor.\n",
    "        \"\"\"\n",
    "        return self.data_tensor.size(0)\n",
    "\n",
    "## instantiate train and test datasets\n",
    "malware_testset = MalwareDataset(test_data[0], test_data[1], test_data[2])\n",
    "malware_trainset = MalwareDataset(train_data[0], train_data[1], train_data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_l0tLsEVsMi0"
   },
   "source": [
    "# 2. Train Model\n",
    "\n",
    "You will now train an ML model on the vectorized datasets you created previously.\n",
    "\n",
    "_Note_: Although we often refer to each input as a 'vector' for simplicity, each of your inputs can also be higher dimensional tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UInY1YzksMi0"
   },
   "source": [
    "## 2.a. Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "VnSv0J75sMi0"
   },
   "outputs": [],
   "source": [
    "# Feel free to edit anything in this block\n",
    "## temporarily upload files to cloud for moving them around: !curl --upload-file ./train_v5700_l2000.pkl https://transfer.sh/train_v5700_l2000.pkl\n",
    "\n",
    "def evaluate_preds(y_gt, y_pred):\n",
    "    recall = recall_score(y_gt, y_pred, average='macro')\n",
    "    return recall\n",
    "\n",
    "\n",
    "def another_helper(question):\n",
    "    return 42\n",
    "\n",
    "\n",
    "def save_model(model, out_path):\n",
    "    pass\n",
    "\n",
    "\n",
    "def pickle_file(out_path, file_content):\n",
    "    with open(out_path, 'wb') as wf:\n",
    "        pickle.dump(file_content, wf)\n",
    "        \n",
    "def unpickle_file(in_path):\n",
    "    return pickle.load(open(in_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl --upload-file ./application_vocab_5701.pkl https://transfer.sh/application_vocab_5701.pkl\n",
    "#pickle_file('train_v5701_bow.pkl', train_data)\n",
    "#pickle_file('test_v5701_bow.pkl', test_data)\n",
    "#application_vocab_5701\n",
    "# !wget https://transfer.sh/uzTwuQ/test_v5701_bow.pkl\n",
    "#!wget https://transfer.sh/WTptcx/application_vocab_5701.pkl\n",
    "# !wget https://transfer.sh/bqXwhn/train_v5701_bow.pkl\n",
    "# train_data = unpickle_file('train_v5701_bow.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhbVzVLDsMi1"
   },
   "source": [
    "## 2.b. Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrUKCNRgsMi1"
   },
   "source": [
    "Describe your model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ILB2fBRWsMi1"
   },
   "outputs": [],
   "source": [
    "# Feel free to edit anything in this block\n",
    "\n",
    "class MalwareNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MalwareNet, self).__init__()\n",
    "        # Layer definitions\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "             nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "             nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QO1yPx7YsMi1"
   },
   "source": [
    "## 2.c. Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "0k0jLX5ksMi1"
   },
   "outputs": [],
   "source": [
    "# Define your hyperparameters here\n",
    "\n",
    "in_dims = train_data[0][0].shape[0]\n",
    "out_dims = len(label_encodings)\n",
    "\n",
    "# Optimization\n",
    "n_epochs = 100\n",
    "batch_size = 512\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10945])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1].shape ####### distribution, hyperparemter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtpgVAO1sMi2"
   },
   "source": [
    "## 2.d. Train your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Co0W2ifGsMi2"
   },
   "outputs": [],
   "source": [
    "# Feel free to edit anything in this block\n",
    "\n",
    "model = MalwareNet(input_dim=in_dims, output_dim=out_dims)\n",
    "model.train()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Data Loaders\n",
    "trainloader = DataLoader(malware_trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(malware_testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "3afVBWVKsMi2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100] Train loss: 8.571567595005035 Recall score: 0.8162219033170013\n",
      "[1/100] Validation Recall score: 0.7647315648975639\n",
      "[2/100] Train loss: 8.605987638235092 Recall score: 0.8084158103366414\n",
      "[2/100] Validation Recall score: 0.7716492380582258\n",
      "[3/100] Train loss: 8.570148408412933 Recall score: 0.8114694401154798\n",
      "[3/100] Validation Recall score: 0.7827801776676473\n",
      "[4/100] Train loss: 8.365752875804901 Recall score: 0.8205727395356448\n",
      "[4/100] Validation Recall score: 0.7521836930767695\n",
      "[5/100] Train loss: 8.401935130357742 Recall score: 0.8176103455807123\n",
      "[5/100] Validation Recall score: 0.7763823505558458\n",
      "[6/100] Train loss: 8.314486414194107 Recall score: 0.819687112451901\n",
      "[6/100] Validation Recall score: 0.7698572649345323\n",
      "[7/100] Train loss: 8.186031192541122 Recall score: 0.8206858282549726\n",
      "[7/100] Validation Recall score: 0.7639874409500983\n",
      "[8/100] Train loss: 8.288580030202866 Recall score: 0.8218369922822761\n",
      "[8/100] Validation Recall score: 0.7600525338048504\n",
      "[9/100] Train loss: 8.347914606332779 Recall score: 0.8184868747921945\n",
      "[9/100] Validation Recall score: 0.7670070232426729\n",
      "[10/100] Train loss: 8.07070741057396 Recall score: 0.8219871735045075\n",
      "[10/100] Validation Recall score: 0.7760284419584392\n",
      "[11/100] Train loss: 8.32776328921318 Recall score: 0.8159637860628637\n",
      "[11/100] Validation Recall score: 0.7804057500780767\n",
      "[12/100] Train loss: 8.303088694810867 Recall score: 0.8183334027675044\n",
      "[12/100] Validation Recall score: 0.7709060509667671\n",
      "[13/100] Train loss: 8.210881769657135 Recall score: 0.8222160693800193\n",
      "[13/100] Validation Recall score: 0.7668015448411051\n",
      "[14/100] Train loss: 8.118473440408707 Recall score: 0.8221734163944397\n",
      "[14/100] Validation Recall score: 0.7770257305840401\n",
      "[15/100] Train loss: 8.144835531711578 Recall score: 0.8229248059748613\n",
      "[15/100] Validation Recall score: 0.7883965222627267\n",
      "[16/100] Train loss: 8.377260357141495 Recall score: 0.8175582496723912\n",
      "[16/100] Validation Recall score: 0.7533783044645423\n",
      "[17/100] Train loss: 8.146336257457733 Recall score: 0.8228568473527226\n",
      "[17/100] Validation Recall score: 0.7648965521291397\n",
      "[18/100] Train loss: 8.242289423942566 Recall score: 0.8158944197917062\n",
      "[18/100] Validation Recall score: 0.7833492174507987\n",
      "[19/100] Train loss: 8.39672064781189 Recall score: 0.8197345929988172\n",
      "[19/100] Validation Recall score: 0.7443989864791924\n",
      "[20/100] Train loss: 8.14209771156311 Recall score: 0.8193528995385584\n",
      "[20/100] Validation Recall score: 0.7703028257508496\n",
      "[21/100] Train loss: 8.060721009969711 Recall score: 0.8221232914764943\n",
      "[21/100] Validation Recall score: 0.7789392641021767\n",
      "[22/100] Train loss: 8.194505959749222 Recall score: 0.8204966891548556\n",
      "[22/100] Validation Recall score: 0.778575689019414\n",
      "[23/100] Train loss: 8.02892953157425 Recall score: 0.8264655977942372\n",
      "[23/100] Validation Recall score: 0.7633502944512169\n",
      "[24/100] Train loss: 7.999661296606064 Recall score: 0.8223242260109747\n",
      "[24/100] Validation Recall score: 0.7784666680863974\n",
      "[25/100] Train loss: 8.44542607665062 Recall score: 0.8198047805472104\n",
      "[25/100] Validation Recall score: 0.7708875857638025\n",
      "[26/100] Train loss: 8.473719328641891 Recall score: 0.8212649027511164\n",
      "[26/100] Validation Recall score: 0.7618314131741826\n",
      "[27/100] Train loss: 8.354760348796844 Recall score: 0.8170138514910587\n",
      "[27/100] Validation Recall score: 0.7866629499876818\n",
      "[28/100] Train loss: 8.142266124486923 Recall score: 0.8205741953644324\n",
      "[28/100] Validation Recall score: 0.7539862175016838\n",
      "[29/100] Train loss: 8.472056925296783 Recall score: 0.8153422842994132\n",
      "[29/100] Validation Recall score: 0.7613693884524363\n",
      "[30/100] Train loss: 8.246416449546814 Recall score: 0.8218066282901167\n",
      "[30/100] Validation Recall score: 0.7760220791596261\n",
      "[31/100] Train loss: 8.374887526035309 Recall score: 0.8217460518546276\n",
      "[31/100] Validation Recall score: 0.7732380721634796\n",
      "[32/100] Train loss: 8.178210645914078 Recall score: 0.8223174692841668\n",
      "[32/100] Validation Recall score: 0.7818219898402331\n",
      "[33/100] Train loss: 8.230905890464783 Recall score: 0.8244383175938712\n",
      "[33/100] Validation Recall score: 0.7640899374493504\n",
      "[34/100] Train loss: 8.534597992897034 Recall score: 0.812475786557272\n",
      "[34/100] Validation Recall score: 0.778478814553721\n",
      "[35/100] Train loss: 8.014631301164627 Recall score: 0.8247819010865369\n",
      "[35/100] Validation Recall score: 0.760278132700917\n",
      "[36/100] Train loss: 8.114565461874008 Recall score: 0.8223572893719855\n",
      "[36/100] Validation Recall score: 0.7840584976799182\n",
      "[37/100] Train loss: 7.882674336433411 Recall score: 0.8217533055796341\n",
      "[37/100] Validation Recall score: 0.785901483711265\n",
      "[38/100] Train loss: 7.907222926616669 Recall score: 0.8246218514588438\n",
      "[38/100] Validation Recall score: 0.7846528635297242\n",
      "[39/100] Train loss: 8.111940413713455 Recall score: 0.8214138103293138\n",
      "[39/100] Validation Recall score: 0.7776539928937479\n",
      "[40/100] Train loss: 7.990300625562668 Recall score: 0.8202382754379531\n",
      "[40/100] Validation Recall score: 0.7796115472442938\n",
      "[41/100] Train loss: 7.854292094707489 Recall score: 0.8254442230286614\n",
      "[41/100] Validation Recall score: 0.779664482894327\n",
      "[42/100] Train loss: 7.838693231344223 Recall score: 0.8285749227058794\n",
      "[42/100] Validation Recall score: 0.7853126915124593\n",
      "[43/100] Train loss: 8.127669543027878 Recall score: 0.8206304637143862\n",
      "[43/100] Validation Recall score: 0.783627466369927\n",
      "[44/100] Train loss: 7.903663069009781 Recall score: 0.8278351775648456\n",
      "[44/100] Validation Recall score: 0.7734936098160922\n",
      "[45/100] Train loss: 8.23015233874321 Recall score: 0.8263051332251354\n",
      "[45/100] Validation Recall score: 0.7899155608103897\n",
      "[46/100] Train loss: 8.353304594755173 Recall score: 0.8209023074279379\n",
      "[46/100] Validation Recall score: 0.7775878197946264\n",
      "[47/100] Train loss: 8.195361763238907 Recall score: 0.8257680698712349\n",
      "[47/100] Validation Recall score: 0.7738359424665076\n",
      "[48/100] Train loss: 7.997180014848709 Recall score: 0.8271089705485588\n",
      "[48/100] Validation Recall score: 0.7871669813601881\n",
      "[49/100] Train loss: 8.295834392309189 Recall score: 0.8141823843144866\n",
      "[49/100] Validation Recall score: 0.7834875101376815\n",
      "[50/100] Train loss: 7.84502387046814 Recall score: 0.8257382492443677\n",
      "[50/100] Validation Recall score: 0.7786254927578261\n",
      "[51/100] Train loss: 7.8502408266067505 Recall score: 0.8285100726628617\n",
      "[51/100] Validation Recall score: 0.7753662377776285\n",
      "[52/100] Train loss: 7.925498336553574 Recall score: 0.8300718680023774\n",
      "[52/100] Validation Recall score: 0.7874777223790226\n",
      "[53/100] Train loss: 7.895887166261673 Recall score: 0.826362542568782\n",
      "[53/100] Validation Recall score: 0.789548074402983\n",
      "[54/100] Train loss: 7.7648506462574005 Recall score: 0.8313994316048777\n",
      "[54/100] Validation Recall score: 0.7618580001824663\n",
      "[55/100] Train loss: 7.982411891222 Recall score: 0.8266271591508223\n",
      "[55/100] Validation Recall score: 0.777683134237382\n",
      "[56/100] Train loss: 7.7286204397678375 Recall score: 0.8333836626615281\n",
      "[56/100] Validation Recall score: 0.7904004776390895\n",
      "[57/100] Train loss: 8.06504201889038 Recall score: 0.8286852907509381\n",
      "[57/100] Validation Recall score: 0.7787122200263676\n",
      "[58/100] Train loss: 8.259490877389908 Recall score: 0.82663602649442\n",
      "[58/100] Validation Recall score: 0.7888735130973952\n",
      "[59/100] Train loss: 7.752492815256119 Recall score: 0.8284052410353807\n",
      "[59/100] Validation Recall score: 0.7585457168721861\n",
      "[60/100] Train loss: 7.907995283603668 Recall score: 0.8266682341726213\n",
      "[60/100] Validation Recall score: 0.7794302363237706\n",
      "[61/100] Train loss: 7.706794589757919 Recall score: 0.8333869134619902\n",
      "[61/100] Validation Recall score: 0.7789209906562801\n",
      "[62/100] Train loss: 7.6684437692165375 Recall score: 0.830178804563017\n",
      "[62/100] Validation Recall score: 0.7894432060461603\n",
      "[63/100] Train loss: 7.631802588701248 Recall score: 0.8351872092409381\n",
      "[63/100] Validation Recall score: 0.7697793750044064\n",
      "[64/100] Train loss: 7.900379091501236 Recall score: 0.8290410016579365\n",
      "[64/100] Validation Recall score: 0.7872267880267315\n",
      "[65/100] Train loss: 7.792852848768234 Recall score: 0.8323628985961269\n",
      "[65/100] Validation Recall score: 0.777160855154135\n",
      "[66/100] Train loss: 7.647003948688507 Recall score: 0.8313374110222158\n",
      "[66/100] Validation Recall score: 0.7864000387780308\n",
      "[67/100] Train loss: 7.791667938232422 Recall score: 0.8324134628687982\n",
      "[67/100] Validation Recall score: 0.7818375766162422\n",
      "[68/100] Train loss: 7.802253186702728 Recall score: 0.8328931559637389\n",
      "[68/100] Validation Recall score: 0.7480368437399588\n",
      "[69/100] Train loss: 7.872848689556122 Recall score: 0.8254835029135617\n",
      "[69/100] Validation Recall score: 0.7864736136469956\n",
      "[70/100] Train loss: 7.741090267896652 Recall score: 0.8335482289166837\n",
      "[70/100] Validation Recall score: 0.7874120399929381\n",
      "[71/100] Train loss: 7.795986592769623 Recall score: 0.832838029089064\n",
      "[71/100] Validation Recall score: 0.7710054310160952\n",
      "[72/100] Train loss: 7.802075356245041 Recall score: 0.8275351667067309\n",
      "[72/100] Validation Recall score: 0.7700708388757151\n",
      "[73/100] Train loss: 7.714521169662476 Recall score: 0.8300825842244295\n",
      "[73/100] Validation Recall score: 0.7854938911691338\n",
      "[74/100] Train loss: 7.561017721891403 Recall score: 0.8335950914463031\n",
      "[74/100] Validation Recall score: 0.7846846082470853\n",
      "[75/100] Train loss: 7.71709281206131 Recall score: 0.8304987007362967\n",
      "[75/100] Validation Recall score: 0.7742221529379918\n",
      "[76/100] Train loss: 7.606970012187958 Recall score: 0.8346771541265472\n",
      "[76/100] Validation Recall score: 0.7809639487901949\n",
      "[77/100] Train loss: 7.559936612844467 Recall score: 0.834407555399768\n",
      "[77/100] Validation Recall score: 0.7721384466850342\n",
      "[78/100] Train loss: 7.577458679676056 Recall score: 0.833258644157862\n",
      "[78/100] Validation Recall score: 0.7878167348401757\n",
      "[79/100] Train loss: 7.547244340181351 Recall score: 0.8360261134243576\n",
      "[79/100] Validation Recall score: 0.7840006235118253\n",
      "[80/100] Train loss: 7.478085368871689 Recall score: 0.8373364438896909\n",
      "[80/100] Validation Recall score: 0.7770673819753798\n",
      "[81/100] Train loss: 7.542987942695618 Recall score: 0.832561127072184\n",
      "[81/100] Validation Recall score: 0.7816222905853806\n",
      "[82/100] Train loss: 7.502257615327835 Recall score: 0.8323620136293993\n",
      "[82/100] Validation Recall score: 0.7885943156449097\n",
      "[83/100] Train loss: 7.553716003894806 Recall score: 0.8375113482211776\n",
      "[83/100] Validation Recall score: 0.7807114385525862\n",
      "[84/100] Train loss: 7.5765712559223175 Recall score: 0.8342843334266599\n",
      "[84/100] Validation Recall score: 0.7850594642155004\n",
      "[85/100] Train loss: 7.520419061183929 Recall score: 0.8355247397470261\n",
      "[85/100] Validation Recall score: 0.7757668734912297\n",
      "[86/100] Train loss: 7.754875332117081 Recall score: 0.8288404503744078\n",
      "[86/100] Validation Recall score: 0.7849032078981113\n",
      "[87/100] Train loss: 7.717191457748413 Recall score: 0.8326409357428274\n",
      "[87/100] Validation Recall score: 0.7654985597981209\n",
      "[88/100] Train loss: 7.553419470787048 Recall score: 0.831252458820477\n",
      "[88/100] Validation Recall score: 0.7808037623034998\n",
      "[89/100] Train loss: 7.485491931438446 Recall score: 0.8363616339741113\n",
      "[89/100] Validation Recall score: 0.7764097753625572\n",
      "[90/100] Train loss: 7.44953253865242 Recall score: 0.8349164776395753\n",
      "[90/100] Validation Recall score: 0.78351469594125\n",
      "[91/100] Train loss: 7.523939788341522 Recall score: 0.8361896521369093\n",
      "[91/100] Validation Recall score: 0.7857286870207518\n",
      "[92/100] Train loss: 7.535214483737946 Recall score: 0.8359102730994217\n",
      "[92/100] Validation Recall score: 0.7797152441824332\n",
      "[93/100] Train loss: 7.544063240289688 Recall score: 0.8373828053529518\n",
      "[93/100] Validation Recall score: 0.7788385766079114\n",
      "[94/100] Train loss: 7.557817816734314 Recall score: 0.8368287684481427\n",
      "[94/100] Validation Recall score: 0.7826087327086386\n",
      "[95/100] Train loss: 7.482937008142471 Recall score: 0.8373171489118418\n",
      "[95/100] Validation Recall score: 0.7829359504457367\n",
      "[96/100] Train loss: 7.569559246301651 Recall score: 0.8332517689039266\n",
      "[96/100] Validation Recall score: 0.7866821231554202\n",
      "[97/100] Train loss: 7.685734808444977 Recall score: 0.8343711992169573\n",
      "[97/100] Validation Recall score: 0.7400337052594501\n",
      "[98/100] Train loss: 7.631689131259918 Recall score: 0.8328316086649605\n",
      "[98/100] Validation Recall score: 0.7803838162820231\n",
      "[99/100] Train loss: 7.578191041946411 Recall score: 0.8332208550814357\n",
      "[99/100] Validation Recall score: 0.7840183909562065\n",
      "[100/100] Train loss: 7.502921789884567 Recall score: 0.8356444741442678\n",
      "[100/100] Validation Recall score: 0.7816692201995721\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example:\n",
    "# for epoch in range(n_epochs):\n",
    "#     ... train ...\n",
    "#     ... validate ...\n",
    "\n",
    "device = device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss() ## since we are doing multiclass classification\n",
    "for epoch in range(n_epochs):\n",
    "    y_true = list()\n",
    "    y_pred = list()\n",
    "    total_loss = 0\n",
    "    for batch, targets, lengths in trainloader:\n",
    "        \n",
    "        ## perform forward pass  \n",
    "        batch = batch.type(torch.FloatTensor).to(device)\n",
    "        pred = model(batch) \n",
    "        preds = torch.max(pred, 1)[1]\n",
    "        \n",
    "        ## accumulate predictions per batch for the epoch\n",
    "        y_pred += list([x.item() for x in preds.detach().cpu().numpy()])\n",
    "        targets = torch.LongTensor([x.item() for x in list(targets)])\n",
    "        y_true +=  list([x.item() for x in targets.detach().cpu().numpy()])\n",
    "        \n",
    "        ## compute loss and perform backward pass\n",
    "        loss = criterion(pred.to(device), targets.to(device)) ## compute loss \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "        ## accumulate train loss\n",
    "        total_loss += loss.item() \n",
    "        \n",
    "    print(f\"[{epoch+1}/{n_epochs}] Train loss: {total_loss} Recall score: {evaluate_preds(y_true, y_pred)}\")\n",
    "    \n",
    "    ## init placeholder for predictions and groundtruth\n",
    "    y_true_val = list()\n",
    "    y_pred_val = list()\n",
    "    ## perform validation pass\n",
    "    for batch, targets, lengths in testloader:\n",
    "        ## perform forward pass  \n",
    "        batch = batch.type(torch.FloatTensor).to(device)\n",
    "        pred = model(batch) \n",
    "        preds = torch.max(pred, 1)[1]\n",
    "        \n",
    "        ## accumulate predictions per batch for the epoch\n",
    "        y_pred_val += list([x.item() for x in preds.detach().cpu().numpy()])\n",
    "        targets = torch.LongTensor([x.item() for x in list(targets)])\n",
    "        y_true_val +=  list([x.item() for x in targets.detach().cpu().numpy()])\n",
    "    print(f\"[{epoch+1}/{n_epochs}] Validation Recall score: {evaluate_preds(y_true_val, y_pred_val)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5w-7O5jsMi2"
   },
   "source": [
    "## 2.e. Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMbzX1h4sMi2"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_GE1BQjsMi3"
   },
   "source": [
    "## 2.f. Save Model + Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Xvo--GIsMi3"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-UQbfCrsMi3"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLD6dur1sMi3"
   },
   "source": [
    "# 3. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QRKxLsPsMi3"
   },
   "source": [
    "## 3.a. Summary: Main Results\n",
    "\n",
    "Summarize your approach and results here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNKqJAoasMi4"
   },
   "source": [
    "## 3.b. Discussion\n",
    "\n",
    "Enter your final summary here.\n",
    "\n",
    "For instance, you can address:\n",
    "- What was the performance you obtained with the simplest approach?\n",
    "- Which vectorized input representations helped more than the others?\n",
    "- Which malwares are difficult to detect and why?\n",
    "- Which approach do you recommend to perform malware classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYGR9ID1sMi4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
