{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4733317f-dcad-40b4-aa6c-5594d8e069f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import optuna\n",
    "import warnings\n",
    "from optuna.visualization import plot_optimization_history, plot_parallel_coordinate, plot_param_importances, plot_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b47f4bb-60e1-4520-9ab0-3c26372a5fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _get_data(DATA_PATH, TRAIN_BATCH_SIZE, TEST_BATCH_SIZE):\n",
    "    try:\n",
    "        mnist_training_dataset = datasets.MNIST(root=DATA_PATH+'train', train=True, download=True, transform=ToTensor())\n",
    "        mnist_testing_dataset = datasets.MNIST(root=DATA_PATH+'test', train=False, download=True, transform=ToTensor())\n",
    "        \n",
    "        training_dataset, validation_dataset = random_split(mnist_training_dataset, [int(0.8*len(mnist_training_dataset)), int(00.2*len(mnist_training_dataset))])\n",
    "        \n",
    "        train_loader = DataLoader(training_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "        validation_loader = DataLoader(training_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=False)\n",
    "        test_loader = DataLoader(mnist_testing_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        return train_loader, validation_loader, test_loader\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Unable to get data due to ', e)\n",
    "\n",
    "        \n",
    "def _network_training(model, train_loader, validation_loader, criterion, optimizer, training_params, tuning=False):\n",
    "    try:\n",
    "        best_accuracy = 0\n",
    "        loss_history = []\n",
    "        training_acc_history = []\n",
    "        validation_acc_history = []\n",
    "        \n",
    "        for epoch in range(0, training_params['NUM_EPOCHS']):\n",
    "            model.train()\n",
    "            loss_scores = []\n",
    "            training_acc_scores = []\n",
    "            correct_predictions = 0\n",
    "            \n",
    "            for batch_index, (images, targets) in enumerate(train_loader):\n",
    "                images = images.to(training_params['DEVICE'])\n",
    "                targets = targets.to(training_params['DEVICE'])\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss_scores.append(loss.item())\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_predictions = (preds==targets).sum().item()\n",
    "                training_acc_scores.append(correct_predictions/targets.shape[0])\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                if not tuning:\n",
    "                    if (batch_index+1) % 100 == 0:\n",
    "                        print(f\"Epoch : [{epoch+1}/{training_params['NUM_EPOCHS']}] | Step : [{batch_index+1}/{len(train_loader)}] | Loss : {loss.item()} \")\n",
    "            \n",
    "            loss_history.append((sum(loss_scores)/len(loss_scores)))\n",
    "            training_acc_history.append((sum(training_acc_scores)/len(training_acc_scores))*100)      \n",
    "            print(f'Epoch : {epoch+1} | Loss : {loss_history[-1]} | Training Accuracy : {training_acc_history[-1]}%')\n",
    "       \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                correct_predictions = 0\n",
    "                validation_acc_scores = []\n",
    "\n",
    "                for images, targets in iter(validation_loader):\n",
    "                    images = images.to(training_params['DEVICE'])\n",
    "                    targets = targets.to(training_params['DEVICE'])\n",
    "                                \n",
    "                    outputs = model(images)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    correct_predictions = (preds == targets).sum().item()\n",
    "                    validation_acc_scores.append(correct_predictions/targets.shape[0])\n",
    "\n",
    "                validation_acc_history.append((sum(validation_acc_scores)/len(validation_acc_scores))*100)\n",
    "                print(f'Epoch {epoch+1} | Validation Accuracy {validation_acc_history[-1]}%')\n",
    "                \n",
    "                if not tuning: \n",
    "                    if validation_acc_history[-1]>best_accuracy:\n",
    "                        best_accuracy = validation_acc_history[-1]\n",
    "                        print('Saving the model...')\n",
    "                        torch.save(model.state_dict(), f\"Accuracy_{best_accuracy}_batchsize_{training_params['BATCH_SIZE']}_lr_{training_params['LEARNING_RATE']}.ckpt\")\n",
    "                \n",
    "        return loss_history\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Error occured in training method = ', e)\n",
    "\n",
    "        \n",
    "def _test_model(BEST_MODEL):\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(BEST_MODEL))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            correct_predictions = 0\n",
    "            test_images = 0\n",
    "            testing_acc_history = []\n",
    "\n",
    "            for images, targets in iter(test_loader):\n",
    "                images = images.to(training_params['DEVICE'])\n",
    "                targets = targets.to(training_params['DEVICE'])\n",
    "\n",
    "                outputs = network(images)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_predictions = (preds==targets).sum().item()\n",
    "                test_images += targets.shape[0]\n",
    "                testing_acc_history.append(correct_predictions/targets.shape[0])\n",
    "\n",
    "            print(f'Accuracy of the network on the {test_images} test images: {(sum(testing_acc_history)/len(testing_acc_history))*100}')\n",
    "        \n",
    "    except Exception as e:\n",
    "            print('Error occured in testing the model = ', e)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1742a790-dd85-4713-a326-121959ef1fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN_Network(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        try:\n",
    "            super(CNN_Network, self).__init__()\n",
    "            \n",
    "            layers = []\n",
    "            \n",
    "            for input_channel, out_channel in zip([model_params['INPUT_SIZE']] + model_params['HIDDEN_LAYERS'][:-1], \n",
    "                                                     model_params['HIDDEN_LAYERS'][:len(model_params['HIDDEN_LAYERS'])]):\n",
    "                layers.append(nn.Conv2d(input_channel, out_channel, model_params['KERNEL'], model_params['STRIDE'], model_params['PADDING'], bias=True))\n",
    "                layers.append(nn.MaxPool2d(2, 2))\n",
    "                layers.append(nn.ReLU())\n",
    "            layers.append(nn.Flatten(1))      \n",
    "            layers.append(nn.Linear(model_params['HIDDEN_LAYERS'][-1], model_params['OUTPUT_SIZE'], bias=True))\n",
    "\n",
    "            self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print('initializing failed due to ', e)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            return self.layers(x)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print('forward pass failed due to ', e)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6acd4e2-1439-498b-a956-e453708a76cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ff79ef2-bc0d-410b-a4c8-84a096537b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-11-04 04:09:32,389]\u001b[0m A new study created in memory with name: no-name-c55aec76-fcbf-4853-8fbb-b443d3e8e556\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 | Loss : 2.2018000485102336 | Training Accuracy : 19.066666666666666%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-11-04 04:09:50,201]\u001b[0m Trial 0 finished with value: 2.2018000485102336 and parameters: {'TRAIN_BATCH_SIZE': 64, 'LEARNING_RATE': 0.00711612809661432, 'OPTIMIZER': <class 'torch.optim.sgd.SGD'>}. Best is trial 0 with value: 2.2018000485102336.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Validation Accuracy 52.21875%\n",
      "Epoch : 1 | Loss : 0.3483588643744588 | Training Accuracy : 88.60833333333333%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-11-04 04:10:04,690]\u001b[0m Trial 1 finished with value: 0.3483588643744588 and parameters: {'TRAIN_BATCH_SIZE': 64, 'LEARNING_RATE': 0.0047752154928028335, 'OPTIMIZER': <class 'torch.optim.adam.Adam'>}. Best is trial 1 with value: 0.3483588643744588.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Validation Accuracy 96.72916666666667%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'analysis_study' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41476/809498523.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_hyper_parameter_tuning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SELECTED BEST SET OF HYPER-PARAMETERS:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0manalysis_study\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LOWEST LOSS SCORE ACHEIVED USING THE BEST HYPER-PARAMTERS\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manalysis_study\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'analysis_study' is not defined"
     ]
    }
   ],
   "source": [
    "def _hyper_parameter_tuning(trial):\n",
    "    \n",
    "    DATA_PATH = 'D:/Repos/MLCS_Project_Assignments/'\n",
    "    \n",
    "    params = {\n",
    "        'DEVICE' : torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        'TRAIN_BATCH_SIZE' :  trial.suggest_categorical('TRAIN_BATCH_SIZE', (32, 64)),\n",
    "        'LEARNING_RATE' : trial.suggest_loguniform('LEARNING_RATE', 0.001, 0.01),\n",
    "        'OPTIMIZER': trial.suggest_categorical('OPTIMIZER', (optim.Adam, optim.SGD)),\n",
    "        'NUM_EPOCHS' : 1\n",
    "    }\n",
    "    \n",
    "    model_params = {\n",
    "        'INPUT_SIZE' : 1,\n",
    "        'HIDDEN_LAYERS' : [160, 100, 64, 10],\n",
    "        'OUTPUT_SIZE' : 10,\n",
    "        'KERNEL' : 3,\n",
    "        'STRIDE' : 1,\n",
    "        'PADDING' : 1\n",
    "    }\n",
    "    \n",
    "    train_loader, validation_loader, _ = _get_data(DATA_PATH, params['TRAIN_BATCH_SIZE'], 1000)\n",
    "    model = CNN_Network(model_params).to(params['DEVICE'])\n",
    "    criterion = nn.CrossEntropyLoss().to(params['DEVICE'])\n",
    "    optimizer = params['OPTIMIZER'](model.parameters(), lr=params['LEARNING_RATE'])\n",
    "    \n",
    "    loss_history = _network_training(model, train_loader, validation_loader, criterion, optimizer, params, tuning=True)\n",
    "    return np.mean(loss_history)\n",
    "\n",
    "with warnings.catch_warnings(record=True):\n",
    "    analysis_study = optuna.create_study(direction='minimize')\n",
    "    analysis_study.optimize(_hyper_parameter_tuning, n_trials=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e049c63d-6bca-4889-9c28-3351476cf0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SELECTED BEST SET OF HYPER-PARAMETERS:\",analysis_study.best_params)\n",
    "print(\"LOWEST LOSS SCORE ACHEIVED USING THE BEST HYPER-PARAMTERS\", analysis_study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232276fa-cf0a-4d09-9863-aaca7cd54923",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = analysis_study.trials_dataframe().drop(['state','datetime_start','datetime_complete','number'], axis=1)\n",
    "df.index.name = 'trial'\n",
    "df.sort_values(by='value').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a5f6d-c53e-4da8-af32-6e8c24cadc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(analysis_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b75d52-dfb7-42b4-9fee-ad05de57f981",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parallel_coordinate(analysis_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e539459c-0acb-4b2c-ae40-78248332410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(analysis_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6162ba6c-153b-4421-857c-d1b9a0f41af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_importances(analysis_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef59d73f-1a13-40a9-a498-39ba298ad040",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__' :\n",
    "    \n",
    "    print(torch.__version__)\n",
    "    \n",
    "    DATA_PATH = 'D:\\Repos\\MLCS_Project_Assignments\\\\'\n",
    "    \n",
    "    training_params = {\n",
    "        'DEVICE' : torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        'TRAIN_BATCH_SIZE' : 64,\n",
    "        'TEST_BATCH_SIZE' : 1000,\n",
    "        'LEARNING_RATE' : 0.001,\n",
    "        'NUM_EPOCHS' : 1\n",
    "    }\n",
    "    \n",
    "    model_params = {\n",
    "        'INPUT_SIZE' : 1,\n",
    "        'HIDDEN_LAYERS' : [160, 100, 64, 10],\n",
    "        'OUTPUT_SIZE' : 10,\n",
    "        'KERNEL' : 3,\n",
    "        'STRIDE' : 1,\n",
    "        'PADDING' : 1\n",
    "    }\n",
    "    \n",
    "    train_loader, validation_loader, test_loader = _get_data(DATA_PATH, training_params['TRAIN_BATCH_SIZE'], training_params['TEST_BATCH_SIZE'])\n",
    "    \n",
    "    model = CNN_Network(model_params).to(training_params['DEVICE'])\n",
    "    print(f'Network structure is: {model.parameters}')\n",
    "    print(f'Total number of parameters: {sum(p.numel() for p in model.parameters())}')\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().to(training_params['DEVICE'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=training_params['LEARNING_RATE'])\n",
    "\n",
    "    loss = _network_training(training_params)\n",
    "    \n",
    "    _test_model(BEST_MODEL='Accuracy_98.875_batchsize_64_lr_0.001.ckpt')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835f3a3f-40af-4a76-bc30-0725be498cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9304ec-d51c-47f0-87d9-cb0b4674dff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef20c45d-2f16-4e58-ab61-f393a7f682e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(targets.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc5d3f-ce87-4f47-b1d2-f72858f5d953",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
